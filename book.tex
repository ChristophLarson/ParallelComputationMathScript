%Book
%Copyright (C) 2019  Patrick Diehl
%
%This program is free software: you can redistribute it and/or modify
%it under the terms of the GNU General Public License as published by
%the Free Software Foundation, either version 3 of the License, or
%(at your option) any later version.

%This program is distributed in the hope that it will be useful,
%but WITHOUT ANY WARRANTY; without even the implied warranty of
%MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
%GNU General Public License for more details.

%You should have received a copy of the GNU General Public License
%along with this program.  If not, see <http://www.gnu.org/licenses/>.

% The template of the book is based on the The Legrand Orange Book
% LaTeX Template
% Version 2.4 (26/09/2018)
% Original author:
% Mathias Legrand (legrand.mathias@gmail.com) with modifications by:
% Vel (vel@latextemplates.com)
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt,fleqn]{book} % Default font size and left-justified equations

\input{structure.tex} % Insert the commands.tex file which contains the majority of the structure behind the template

\hypersetup{pdftitle={Parallel Computational Mathematics},pdfauthor={Patrick Diehl, Phd}} % Uncomment and fill out to include PDF metadata for the author and title of the book

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begingroup
\thispagestyle{empty} % Suppress headers and footers on the title page
\begin{tikzpicture}[remember picture,overlay]
%\node[inner sep=0pt] (background) at (current page.center) {\includegraphics[width=\paperwidth]{background.pdf}};
\draw (current page.center) node [fill=azure!30!white,fill opacity=0.6,text opacity=1,inner sep=1cm]{\Huge\centering\bfseries\sffamily\parbox[c][][t]{\paperwidth}{\centering Parallel Computaitonal Mathematics\\[15pt] % Book title
{\Large Fall 2020}\\[20pt] % Subtitle
{\huge Dr. Patrick Diehl}}}; % Author name
\end{tikzpicture}
\vfill
\endgroup

%----------------------------------------------------------------------------------------
%	COPYRIGHT PAGE
%----------------------------------------------------------------------------------------

\newpage
~\vfill
\thispagestyle{empty}

\noindent Copyright \copyright\ 2020 Patrick Diehl\orcid{0000-0003-3922-8419}\\ % Copyright notice

%\noindent \textsc{Published by Publisher}\\ % Publisher

\noindent \url{https://www.cct.lsu.edu/~pdiehl/teaching/2020/4997/}\\ % URL

\noindent \doclicenseThis  

\noindent \textit{Edition, Fall 2020 \tiny{(git commit \input{{.git/refs/heads/master}}})} % Printing/edition date

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

%\usechapterimagefalse % If you don't want to include a chapter image, use this to toggle images off - it can be enabled later with \usechapterimagetrue

%\chapterimage{chapter_head_1.pdf} % Table of contents heading image

\pagestyle{empty} % Disable headers and footers for the following pages

\tableofcontents % Print the table of contents itself

\cleardoublepage % Forces the first chapter to start on an odd page so it's on the right side of the book

\pagestyle{fancy} % Enable headers and footers again

\chapter*{Forward}

%----------------------------------------------------------------------------------------
%	PART
%----------------------------------------------------------------------------------------

\part{Introduction: C++ and the C++ standard template library}

%----------------------------------------------------------------------------------------
%	CHAPTER 1
%----------------------------------------------------------------------------------------

%\include{sections/chapter1}

%----------------------------------------------------------------------------------------
\part{Linear algebra and solvers}
%----------------------------------------------------------------------------------------

%\include{sections/chapter2}

%----------------------------------------------------------------------------------------
\part{Numerical examples}
\label{part:numerical:examples}
%----------------------------------------------------------------------------------------

%\include{sections/chapter3}

%----------------------------------------------------------------------------------------
\part{Parallel and distributed computing}
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
\chapter{Parallel computing}
%----------------------------------------------------------------------------------------
In this Chapter, a brief overview of the technical aspects of parallel computing is given. Note that this course focuses on the implementation details, like asynchronous programming, see Chapter~\ref{sec:async:coding}; parallel algorithms, see Section~\ref{sec:stl:parallel:algorithms}; and the C++ standard library for parallelism and concurrency (HPX), see Chapter~\ref{sec:hpx}. However, we provide some details and further references for the technical aspects and hardware details.\\

Let us begin with a definition of parallelism: \textit{1)} we need multiple resources which can operate at the same, \textit{2)} we have to have more than one task that can be performed at the same time, \textit{3)} we have to do multiple tasks on multiple resources the same time. First, we have to have multiple resources, \emph{e.g.}\ multiple threads of a computation node at the same time. However, with current hardware architecture this is not an issue. Second, this part is more interesting, since we need some code which is independent of each other and can be executed concurrent. Third, here we want to have overlapping computations and communication on multiple resources. For more details about parallel computing, we refer to~\cite{grama2003introduction,trobec2018introduction}.\\

For the second part of the definition, Amdahl's law\index{Amdahl's law}~\cite{amdahl1967validity} or strong scaling\index{scaling} is important. Amdahl's law is given as
\begin{align}
S = \frac{1}{(1-P) + \frac{P}{N}}
\end{align}
where $S$ is the speed up, $P$ the proportion of parallel code, and $N$ the numbers of threads. Figure~\ref{fig:amdals:law} plots Amdahl's law for different ratios of parallel code. Obviously, for zero percent parallel code, there is no speedup. If the portion to parallel code increases, the speedup increases up to a certain amount of threads. Therefore, the parallel computing with many threads is only beneficial for highly parallelism in our program. For example if our code took 20 hours using a single thread to complete and there in a part of one hour which can not be executed in parallel. Thus, only 19 hours of execution time can be parallized $(p=0.95)$ and independent of the amount of threads we use the theoretical speedup is limited to $S=\sfrac{1}{(1-p)}=20$.\\

\begin{figure}[tb]
\centering
 \begin{tikzpicture}
        \begin{axis}[
            no markers,
            samples=100,
            /pgf/declare function={
            f(\x,\a) = 1. / ((1.-\a)+(\a/x));
       		},
       		grid=both,
       		xlabel=$N$ number of threads,
       		ylabel=$S$ speedup,
       		legend pos=outer north east,
        ]
            % ... and use it here
            \addplot+ [domain=1:2048,azure,thick] {f(x,0)};
            \addlegendentry{$P=0\%$}
            \addplot+ [domain=1:2048,cadetgrey,thick] {f(x,0.5)};
            \addlegendentry{$P=50\%$}
            \addplot+ [domain=1:2048,darkgreen,thick] {f(x,.75)};
            \addlegendentry{$P=75\%$}
            \addplot+ [domain=1:2048,amaranth,thick] {f(x,.90)};
            \addlegendentry{$P=90\%$}
            \addplot+ [domain=1:2048,black,thick] {f(x,.95)};
            \addlegendentry{$P=95\%$}
       \end{axis}
    \end{tikzpicture}
\caption{Plot of Amdahl's law for different parallel portions of the code.}
\label{fig:amdals:law}
\end{figure}

Before we look into different parallelism approaches, we look into the example how to compute the dot product $S = \mathbf{X} \cdot \mathbf{V} = \sum_i^N x_i y_i$ of two vectors $\mathbf{X} = \lbrace x_1,x_2,\ldots,x_n \rbrace$ and $\mathbf{Y} = \lbrace y_1,y_2,\ldots,y_n \rbrace$  in a sequential manner and extend this example to the various parallelism approaches. So we have to compute $S = (x_1y_1) + (x_2y_2) + \ldots + (x_n y_n)$ as shown in the flow chart in Figure~\ref{fig:flow:chart:dot:seq}. In the sequential processing, the first to elements of each vector are multiplied $x_1 \times y_1$ and added to the temporal result. After that the second elements are multiplied and added to the temporal result, and so on. \\

\begin{figure}
\centering
\begin{tikzpicture}
\draw (0,0) circle [radius=0.3] node {$\times$};
\draw (2,0) circle [radius=0.3] node {$\times$};
\draw (4,0) circle [radius=0.3] node {$\times$};
\draw (6,0) circle [radius=0.3] node {$\times$};
\draw (10,0) circle [radius=0.3] node {$\times$};
\node[above] at (8,0) {$\ldots$} ;

\draw (2,-1) circle [radius=0.3] node {$+$};
\draw (4,-1) circle [radius=0.3] node {$+$};
\draw (6,-1) circle [radius=0.3] node {$+$};
\draw (10,-1) circle [radius=0.3] node {$+$};
\node[above] at (8,-1) {$\ldots$} ;

\draw[->](0,-0.3) -- (1.7,-1);
\draw[->](2,-0.3) -- (2,-0.7);
\draw[->](4,-0.3) -- (4,-0.7);
\draw[->](6,-0.3) -- (6,-0.7);
\draw[->](10,-0.3) -- (10,-0.7);

\draw[->](2.3,-1) -- (3.7,-1.);
\draw[->](4.3,-1) -- (5.7,-1.);
\draw[->](6.3,-1) -- (7.7,-1.);
\draw[->](8.3,-1) -- (9.7,-1.);

\draw (-0.15,0.3) -- (-0.3,0.6);
\node[above] at (-0.3,0.6) {$x_1$} ;
\draw (2-0.15,0.3) -- (2-0.3,0.6);
\node[above] at (2-0.3,0.6) {$x_2$} ;
\draw (4-0.15,0.3) -- (4-0.3,0.6);
\node[above] at (4-0.3,0.6) {$x_3$} ;
\draw (6-0.15,0.3) -- (6-0.3,0.6);
\node[above] at (6-0.3,0.6) {$x_4$} ;
\draw (10-0.15,0.3) -- (10-0.3,0.6);
\node[above] at (10-0.3,0.6) {$x_n$} ;

\draw (0.15,0.3) -- (0.3,0.6);
\node[above] at (0.3,0.6) {$y_1$} ;
\draw (2+0.15,0.3) -- (2+0.3,0.6);
\node[above] at (2+0.3,0.6) {$y_2$} ;
\draw (4+0.15,0.3) -- (4+0.3,0.6);
\node[above] at (4+0.3,0.6) {$y_3$} ;
\draw (6+0.15,0.3) -- (6+0.3,0.6);
\node[above] at (6+0.3,0.6) {$y_4$} ;
\draw (10+0.15,0.3) -- (10+0.3,0.6);
\node[above] at (10+0.3,0.6) {$y_n$} ;

\draw[->](10,-1.3) -- (10,-1.6);
\node[below] at (10,-1.6) {$s$} ;
\end{tikzpicture}
\caption{Flow chart of the sequential evaluation of the dot product of two vectors. }
\label{fig:flow:chart:dot:seq}
\end{figure}

The first parallelism approach is the pipeline parallelism\index{pipeline parallelism}~\cite{ramamoorthy1977pipeline}. The pipeline parallelism is used in vector processors and in execution pipelines in all general microprocessors. Let us look into some example of from the automotive industry. First, the body of the car is assembled. Second, workers assemble the chassis. Third, workers add the engine into the chassis. Next, the steering wheel is added and many more steps until the car is finally assembled. TO make this process efficient, the workers assembling the chassis do not wait until the last step is finalized before they start working on the next chassis. Side note this is similar to the assembly line introduced bu Henry Ford to enable mass production of cars~\cite{watts2009people}.\\

Figure~\ref{fig:dataflow:pipeline} shows the data flow chart for the pipeline parallelism. In the first step, the values $x_1$ and $y_1$ are read from memory. In the second step the values are multiplied. In the last step the result of the multiplication is added to the variable $S$. However, the other threads do not idle until the result is computed and do a previous step if possible. Meaning if the multiplication at stage two is happening, another thread starts to get the next values. For more details, we refer to~\cite{quinn2003parallel}. \\


\begin{figure}[tb]
\centering
\begin{tikzpicture}
\draw (0,0) rectangle ++(0.75,0.75) node[pos=.5] {$+S$};
\draw (1.5,0) rectangle ++(0.75,0.75) node[pos=.5] {$xy$}; 
\draw (3.5,0) rectangle ++(2,0.75) node[pos=.5] {\text{get} $x_i$,$y_i$};

\node[above] at (8,0.5*0.75) {$\mathbf{X} = \lbrace x_1,x_2,\ldots,x_n \rbrace$} ;
\node[below] at (8,0.5*0.75) {$\mathbf{Y} = \lbrace y_1,y_2,\ldots,y_n \rbrace$} ;
\node at (-1,0.5*0.75) {$S$} ;

\draw[<-](-0.8,0.5*0.75) -- (0,0.5*0.75) ;
\draw[<-](0.75,0.5*0.75) -- (1.5,0.5*0.75) ;
\draw[<-](2.25,0.5*0.75) -- (3.5,0.5*0.75) ;
\draw[<-](5.75,0.8*0.75) -- (6,0.8*0.75) ;
\draw[<-](5.75,0.2*0.75) -- (6,0.2*0.75) ;
\end{tikzpicture}
\caption{Flow chart for the pipeline processing for the dot product.}
\label{fig:dataflow:pipeline}
\end{figure}

The second parallelism approach is the Single instructions and multiple data (SIMD)\index{SIMD}\index{single instructions and multiple data}. SIMD is part of Flynn's taxonomy, a classification of computer architectures, proposed by Michael J. Flynn in 1966~\cite{flynn1972some,duncan1990survey}. Following aspects are relevant 
\begin{itemize}
\item All perform same operation at the same time
\item But may perform different operations at different times
\item Each operates on separate data
\item Used in accelerators on microprocessors
\item Scales as long as data scales.
\end{itemize}
\vspace{0.25cm}
Figure~\ref{fig:reduction:tree:simd} shows the reduction tree for the dot product computation. For this parallelism approach all threads perform the same operation at the same time. In our case all available threads multiply two values at the first level. Second one of these threads add the partial results. Until not all elements are read from the vector these steps are repeated. The last step is to accumulate all partial results and the final result is available. For example previous CUDA architectures were designed this way and introducing branching had some effect on the performance. Newer CUDA architectures perform better here and these things are explained in following talk\link{https://youtu.be/5vr7ItjyIH8}.

\begin{figure}[tb]
\centering
  \begin{tikzpicture}
   \draw (0,0) rectangle ++(0.75,0.75) node[pos=.5] {$P_1$}; 
   \draw (1.5,0) rectangle ++(0.75,0.75) node[pos=.5] {$P_2$}; 
   \draw (3,0) rectangle ++(0.75,0.75) node[pos=.5] {$P_3$}; 
   \draw (4.5,0) rectangle ++(0.75,0.75) node[pos=.5] {$P_4$}; 
   
   \draw (0.75,-2) rectangle ++(0.75,0.75) node[pos=.5] {$+$}; 
   \draw (3.75,-2) rectangle ++(0.75,0.75) node[pos=.5] {$+$}; 
   
   \draw (2.25,-4) rectangle ++(0.75,0.75) node[pos=.5] {$+$}; 
   
   \draw[->] (0.75+0.5*0.75,-2) -- (2.25,-4+0.75) ;
   \draw[->] (3.75+0.5*0.75,-2) -- (3,-4+0.75) ;
   
   \draw[->] (1.5+0.5*0.75,0) -- (1.5,-2+0.75) ;
   \draw[->] (0.5*0.75,0) -- (0.75,-2+0.75) ; 
   
   \draw[->] (3+0.5*0.75,0) -- (3.75,-2+0.75) ;
   \draw[->] (4.5+0.5*0.75,0) -- (4.5,-2+0.75) ; 
   
   \draw[->] (2.25+0.5*0.75,-4) -- (2.25+0.5*0.75,-4.5) ; 
   
   \draw[dashed] (0,-0.75) -- (5,-0.75) ;
   \node[below,rotate=90] at (-.75,-2) {\small Reduction tree}; 
   
   \node at (0.5*0.75,1.5) {\tiny $X=\lbrace x_1,x_2 \rbrace$};
   \node at (0.5*0.75,1.25) {\tiny $Y=\lbrace x_9,x_{10} \rbrace$};
   \node at (1.5+0.5*0.75,1.5) {\tiny $X=\lbrace x_3,x_4 \rbrace$};
   \node at (1.5+0.5*0.75,1.25) {\tiny $Y=\lbrace x_{11},x_{12} \rbrace$};
   \node at (3.+0.5*0.75,1.5) {\tiny $X=\lbrace x_5,x_6 \rbrace$};
   \node at (3.+0.5*0.75,1.25) {\tiny $Y=\lbrace x_{13},x_{14} \rbrace$}; 
   \node at (4.5+0.5*0.75,1.5) {\tiny $X=\lbrace x_7,x_8 \rbrace$};
   \node at (4.5+0.5*0.75,1.25) {\tiny $Y=\lbrace x_{15},x_{16} \rbrace$};
   
   \draw[->] (0.5*0.75,1.125) -- (0.5*0.75,0.75) ;
   \draw[->] (1.5+0.5*0.75,1.125) -- (1.5+0.5*0.75,0.75) ;
   \draw[->] (3.+0.5*0.75,1.125) -- (3.+0.5*0.75,0.75) ;
   \draw[->] (4.5+0.5*0.75,1.125) -- (4.5+0.5*0.75,0.75) ;
   \end{tikzpicture}
   \caption{Reduction tree for the dot product using single instructions and multiple data.}
   \label{fig:reduction:tree:simd}
\end{figure}

%----------------------------------------------------------------------------------------
\subsection*{Memory access}
\index{memory access}
%----------------------------------------------------------------------------------------
For parallel computing, the memory access scheme is important to understand performance behavior. If we initialize for example the two vectors in the dot product example, some space in the memory is reserved and filled with the values. For the computation of the dot product these elements have to be read from memory and the CPU is doing the computation. In a layman's view the CPU is connected to the memory via a so-called bus. Depending on the bus's architecture the access time differs and may have effects on the performance if there is a switch from one CPU to the second CPU.\\

The first memory access scheme is uniform memory access (UMA), see Figure~ref{fig:memory:uma},  where all memory is attached to one bus and all CPU are attached to the same bus. Therefore, the memory access times are the same for all CPU. So we do not see any effect if we switch from one two two CPU. The second memory access scheme is non-uniform memory access (NUMA), see Figure~\ref{fig:memory:numa}. Here, the access time to the memory depends on the memory location relative to the CPU. Thus, local memory access is fast and non-local memory access has some overhead. For more details about memory access, we refer to~\cite{el2005advanced,hager2010introduction}.

\begin{figure}
\centering
\begin{minipage}[c]{0.45\textwidth}
\centering
   \begin{tikzpicture}
%Threads
\draw (0,3) rectangle (.5,3.5) node[pos=.5] {1};
\draw (0.5,3) rectangle (1.0,3.5) node[pos=.5] {..};
\draw (1.,3) rectangle (1.5,3.5) node[pos=.5] {n};

\draw (2.5,3) rectangle (3.,3.5) node[pos=.5] {1};
\draw (3.,3) rectangle (3.5,3.5) node[pos=.5] {..};
\draw (3.5,3) rectangle (4,3.5) node[pos=.5] {n};

\draw (0.75,2.5) -- (0.75,3.);
\draw (3.25,2.5) -- (3.25,3.);

%BUS
\draw (0,1) rectangle (4,1.5) node[pos=.5] {Bus};


%CPU
\draw (0,2) rectangle (1.5,2.5) node[pos=.5] {CPU 1};
\draw (2.5,2) rectangle (4,2.5) node[pos=.5] {CPU 2};
\draw (0.75,2) -- (0.75,1.5);
\draw (3.25,2) -- (3.25,1.5);

%Memory
\draw (0,0) rectangle (4,0.5) node[pos=.5] {Memory};
\draw (0.75,.5) -- (0.75,1.);
\draw (3.25,.5) -- (3.25,1.);
\end{tikzpicture}
    \caption{Uniform memory access (UMA)\index{uniform memory access}\index{UMA}}
    \label{fig:memory:uma}
\end{minipage}
\hfill
\begin{minipage}[c]{0.45\textwidth}
\begin{tikzpicture}
%Threads
\draw (0,3) rectangle (.5,3.5) node[pos=.5] {1};
\draw (0.5,3) rectangle (1.0,3.5) node[pos=.5] {..};
\draw (1.,3) rectangle (1.5,3.5) node[pos=.5] {n};

\draw (2.5,3) rectangle (3.,3.5) node[pos=.5] {1};
\draw (3.,3) rectangle (3.5,3.5) node[pos=.5] {..};
\draw (3.5,3) rectangle (4,3.5) node[pos=.5] {n};

\draw (0.75,2.5) -- (0.75,3.);
\draw (3.25,2.5) -- (3.25,3.);

%BUS
\draw (0,1) rectangle (1.5,1.5) node[pos=.5] {Bus};
\draw (2.5,1) rectangle (4,1.5) node[pos=.5] {Bus};

%CPU
\draw (0,2) rectangle (1.5,2.5) node[pos=.5] {CPU 1};
\draw (2.5,2) rectangle (4,2.5) node[pos=.5] {CPU 2};
\draw (0.75,2) -- (0.75,1.5);
\draw (3.25,2) -- (3.25,1.5);

%Memory
\draw (0,0) rectangle (1.5,0.5) node[pos=.5] {Memory};
\draw (2.5,0) rectangle (4,0.5) node[pos=.5] {Memory};
\draw (0.75,.5) -- (0.75,1.);
\draw (3.25,.5) -- (3.25,1.);
\end{tikzpicture}
    \caption{Non-uniform memory access (NUMA)\index{non-uniform memory access}\index{NUMA}}
    \label{fig:memory:numa}
\end{minipage}

\end{figure}



%----------------------------------------------------------------------------------------
\section{Caution: Data races and dead locks}
%----------------------------------------------------------------------------------------
Remember with great power comes great responsibility! Meaning with shared memory parallelism you add an additional source of error to your code. When using parallel execution policy, it is the programmer's responsibility to avoid
\begin{itemize}
\item data races
\item race conditions
\item deadlocks.
\end{itemize} 
Let us look into some code examples for these kind of errors. A data race\index{data race} exists when multithreaded (or otherwise parallel) code that would access a shared resource could do so in such a way as to cause unexpected results. Listing~\ref{code:data:race} shows an example for a data race for the variable \cpp{sum}. Since the parallel execution policy is used, multiple threads can access the variable \cpp{sum} at the same time which means that not all threads can write to the variable. Thus, the result is might not correct. There are two solutions to avoid the data race. First, the atomic library\link{https://en.cppreference.com/w/cpp/atomic/atomic}. The atomic library\footnote{\tiny\url{https://en.cppreference.com/w/cpp/atomic}} provides components for fine-grained atomic operations allowing for lockless concurrent programming. Each atomic operation is indivisible with regards to any other atomic operation that involves the same object. Atomic objects are free of data races. Listing~\ref{code:datarace:atomic} shows the solution using \cpp{std::atomic:<int>}\link{https://en.cppreference.com/w/cpp/atomic/atomic}. The second solution is shown in Listing~\ref{code:datarace:mutex}. Here, the \cpp{std::mutex} class is used to avoid the data race. The mutex class\link{https://en.cppreference.com/w/cpp/thread/mutex} is a synchronization primitive that can be used to protect shared data from being simultaneously accessed by multiple threads. In Line~4 a \cpp{std::mutex m;} is generated. In Line~8 the lock of the code is started by using \cpp{m.lock();} and in Line~10 the lock is released by using \cpp{m.unlock();}.

\begin{exercise}
Give a definition for \cpp{std::atomic} and \cpp{std::mutex} in your own words. 
\end{exercise}

Another source of error is the race condition\index{race condition} where a check of a shared variable within a parallel execution and another thread could change this variable before it is used. Listing~\ref{code:racecondition} shows the solution to avoid the race condition.  Imagine the code without the \cpp{std::mutex} and the implication to get a wrong result. In the code there is a check if \cpp{x} is equal to 5 and a special treatment of the computation in this case. Now in Line~4 it was true that \cpp{x} was equal to five and the thread enters the \cpp{if} branch. However, in between another thread could change the value of \cpp{x} and not \cpp{y = 5 *2} is computed. By using the mutex this situation is avoided.

\begin{exercise}
Explain a data race in your own words and explain why a \cpp{std::mutex} avoids the data race.
\end{exercise}

\begin{lstlisting}[language=c++,caption={Example code and Solution for a data race.\label{code:data:race}},float,floatplacement=tb]
//Compute the sum of the array a in parallel
int a[] = {0,1,2,3,4};
int sum = 0;
std::for_each(std::execution::par, 
              std::begin(a), 
              std::end(a), [&](int i) {
  sum += a[i]; // Error: Data race
});
\end{lstlisting}


\begin{lstlisting}[language=c++,caption={Solution to avoid the data race using \cpp{std::atomic}.\label{code:datarace:atomic}},float,floatplacement=tb]
//Compute the sum of the array a in parallel
int a[] = {0,1};
std::atomic<int> sum{0};
std::for_each(std::execution::par, 
              std::begin(a), 
              std::end(a), [&](int i) {
  sum += a[i]; 
});
\end{lstlisting}

\begin{lstlisting}[language=c++,caption={Solution to avoid the data race using \cpp{std::mutex}.\label{code:datarace:mutex}},float,floatplacement=tb]
//Compute the sum of the array a in parallel
int a[] = {0,1};
int sum = 0;
std::mutex m;
std::for_each(std::execution::par, 
              std::begin(a), 
              std::end(a), [&](int i) {
  m.lock();
  sum += a[i];
  m.unlock(); 
});
\end{lstlisting}


\begin{lstlisting}[language=c++,caption={Example for the race condition.\label{code:racecondition}},float,floatplacement=tb]
std::mutex m;

m.lock();
if (x == 5)  // Checking x
{
   // Different thread could change x 
      
   y = x * 2; // Using x
}
m.unlock();
// Now it is sure that y will be 10
\end{lstlisting}


A deadlock\index{deadlock} describes a situation where two or more threads are blocked forever and waiting for each others. Following example taken from\link{https://docs.oracle.com/javase/tutorial/essential/concurrency/deadlock.html} explains a deadlock nicely. \\

\textit{Alphonse and Gaston are friends, and great believers in courtesy. A strict rule of courtesy is that when you bow to a friend, you must remain bowed until your friend has a chance to return the bow. Unfortunately, this rule does not account for the possibility that two friends might bow to each other at the same time.}

\begin{exercise}
The implementation of this examples is available on GitHub\link{https://github.com/diehlpkteaching/ParallelComputationMathExamples/blob/master/chapter10/lecture6-deadlock.cpp.ipynb}. Play around with the example and try to understand why the code results in a deadlock. 
\end{exercise}

%----------------------------------------------------------------------------------------
\chapter{Asynchronous programming}
\label{sec:async:coding}
%----------------------------------------------------------------------------------------
A different concept for shared memory parallelism is asynchronous programming~\cite{williams2012c++}. Before we look into asynchronous programming, we look again into the concept of serial programming. Figure~\ref{fig:async:dependencygraph} shows the dependency graph for one computation and one can see that we can compute $P$ and $X$ independent and only $H$ depends on both of them. Listing~\ref{code:serial:dependency} shows the serial computation of the dependency graph. Each line of code is executed line by line Each time a function is called the code waits until the function finishes. Thus, we can not compute $P$ and $X$ independently, even if the data is independent. 

\vspace{0.25cm}
\begin{minipage}{\linewidth}
\begin{minipage}{0.45\linewidth}
\centering
\begin{tikzpicture}
\node (a) [draw,circle] at (0,2) {$H$};
\node (b) [draw,circle] at (0,0) {$P$};
\node (c) [draw,circle] at (1,0) {$X$};
\draw [->] (a) -- (b);
\draw [->] (a) -- (c);
\end{tikzpicture}
\captionof{figure}{Example dependency graph}
\label{fig:async:dependencygraph}
\end{minipage}
\hfill
\begin{minipage}{0.5\linewidth}
\begin{lstlisting}[language=c++,caption={Synchronous execution of the dependency graph.\label{code:serial:dependency}}]
auto P = compute();
auto X = compute();
auto H = compute(P,X);
\end{lstlisting}
\end{minipage}
\end{minipage}
\vspace{0.25cm}

To executed lines asynchronously the C++ language provides the \cpp{std::async}\link{https://en.cppreference.com/w/cpp/thread/async}\index{async}\index{C++!async} expression provided by the \cpp{#include <future>}. Listing~\ref{code:serial:dependency} shows the asynchronous implementation of the dependency graph in Figure~\ref{fig:async:dependencygraph}. Line~2 shows the usage of \cpp{std::async} for the function \cpp{compute}. The first argument is the name of the function or a lambda expression, see Section~\ref{sec:lambda:function}. Because we used \cpp{std::async} this line of code is executed in the background on a different thread and the next line of code is executed, even if the result of the computation is not ready yet. Therefore, \cpp{std::async} return a \cpp{std::future<int>}\link{https://en.cppreference.com/w/cpp/thread/future}\index{future}\index{C++!future} object provided by the \cpp{#include <future>} header which is a template and contains the return type of the function which is in this example the \cpp{int} data type. In Line~4, the computation of $X$ is started on another thread. Such that both computations happens at the same time. In Line~7--9 the results of the asynchronous function call are gathered, since these are needed to compute $H$. With the \cpp{.get()} function a barrier is introduced and the line of codes waits until the computation is ready. In our case, we can wait since we need the two results to compute the last one. Meaning that Line~8 is only executed if the computation in Line~2 has finished. Following synchronization features are available:
\begin{itemize}
\item \lstinline|.get()| returns the result of the functions and wait until the computation finished
\item \lstinline|.wait()|\link{https://en.cppreference.com/w/cpp/thread/future/wait} waits until the computation finished
\item \lstinline|.wait_for(std::chrono::seconds(1))|\link{https://en.cppreference.com/w/cpp/thread/future/wait_for} returns if it is not available for the specified timeout duration 
\item \lstinline|.wait_until(std::chrono::seconds(1))|\link{https://en.cppreference.com/w/cpp/thread/future/wait_until} waits for a result to become available. It blocks until specified timeout time has been reached or the result becomes available, whichever comes first. 
\end{itemize}

\begin{lstlisting}[language=c++,caption={Asynchronous execution of the dependency graph.\label{code:serial:dependency}},float,floatplacement=tb]
// Compute P
std::future<int> f1 = std::async(compute);
// Compute X
auto f2 = std::async(compute);

// Gather the results
int P = f1.get();
int X = f2.get();

// Compute the dependent result H
std::cout << compute(P,X) << std::endl;
\end{lstlisting}

\section*{Example}
Let us look into one example to show the parallelism using asynchronous programming for the Taylor series. The approximation of the $\sin$ function is given as
\begin{align}
\sin(x) = \sum\limits_{n=0}^N (-1)^{n-1} \frac{x^{2n}}{(2n)!} 
\label{eq:sin:taylor}
\end{align}
One approach to parallize the above function using two threads is:
\begin{enumerate}
\item Split $n$ into slices, e.g. 2 times $\sfrac{n}{2}$ for two threads
\item Start two times \lstinline|std::async| where each thread computes $\sfrac{n}{2}$
\item Use the two futures to synchronize the results
\item Combine the two futures to obtain the result.
\end{enumerate}
To distribute $n$ into slices, we need to write the sum in Equation~\eqref{eq:sin:taylor} as
\begin{align}
\sum\limits_{n=begin}^end (-1)^{n-1} \frac{x^{2n}}{(2n)!} \text{.}
\end{align}
Listing~\ref{code:async:taylor} shows how to implement the function to splice the computation of the Taylor series, see Line~5--14. In Line~18--19 the two splices $\sfrac{n}{2}$ are launched from $0$ up to $49$ on the first thread and from $50$ up to $99$ on the second thread. In Line~22 the result is gathered and finally the accumulated result is evaluated. For more details, we refer to following talk\link{https://www.youtube.com/watch?v=js-e8xAMd1s}.

To compile the code using asynchronous programming, we need to add \lstinline|-pthread| to our compiler to use the POSIX threads to launch the functions asynchronous (\lstinline|std::async|). More details about POSIX\index{POSIX} threads~\cite{butenhof1997programming,kleiman1996programming}.




\begin{lstlisting}[language=c++,caption={Asynchronous computation of the $\sin$ function using a Taylor series.\label{code:async:taylor}},float,floatplacement=tb]
#include <future>
#include <iostream>

// Function to compute portion of the Taylor series
double taylor(size_t begin, size_t end, 
double x,size_t n){
double res = 0;

	for( size_t i = begin ; i < end ; i++)
	{
	  res += pow(-1,i-1) * pow(x,2*n) / factorial(2*n);
	} 
return res;
}

int main(){
	// Asynchronous computation using two slices
	auto f1 = std::async(taylor,0,49,2,100); 
	auto f2 = std::async(taylor,50,99,2,100); 
	
	// Gather the result
	double result = f1.get() + f2.get();

	// Print the result
	std::cout << "sin(2)= " res << std::endl;

	return 0;
}
\end{lstlisting}


%----------------------------------------------------------------------------------------
\chapter{Distributed memory}
%----------------------------------------------------------------------------------------


\newpage
\theendnotes


%----------------------------------------------------------------------------------------
\part{HPX}
\label{sec:hpx}
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
\chapter{Introduction to HPX}
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
\section{Parallel algorithms}
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
\section{Asynchronous programming}
%----------------------------------------------------------------------------------------
HPX provides the same features as the C++ language for asynchronous programming, see Chapter~\ref{sec:async:coding} for more details. In this section, we show how to use HPX's function instead of \cpp{std::future} and \cpp{std::async}, since HPX provides more flexibility here. As a disclaimer this is really easy, since we can use the code of the previous example and just replace the name space \cpp{std} with the name space \cpp{hpx}. Listing~\ref{code:hpx:future} shows an example of the example for computing the square number of a asynchronously. In Line~2 the header \cpp{#include <hpx/incldue/lcos.hpp>} is needed to use \cpp{hpx::future} and \cpp{hpx::async}\link{https://stellar-group.github.io/hpx/docs/sphinx/latest/html/examples/fibonacci_local.html?highlight=async}. In Line~12 the function \cpp{square} is called asynchronously using \cpp{hpx::async(square,10)}. Note that the first argument is the name of the function and the second one the function argument. The function call return a \cpp{hpx::future<int>} since the return type of the function is \cpp{int}. To access the result of the function, if the computation has finished the function \cpp{.get()} is used. Note that the only difference here is not to include the header \cpp{#include <future>} and use \cpp{hpx::future} instead of \cpp{std::future} and same for \cpp{hpx:async} instead of \cpp{std::async}. Thus, it is really easy to switch between HPX and C++ for asynchronous programming.

\begin{lstlisting}[language=c++,caption={Asynchronous computation of the square number using HPX.\label{code:hpx:future}},float,floatplacement=tb]
#include <hpx/hpx_init.hpp>
#include <hpx/incldue/lcos.hpp>
#include <iostream>

int square(int a)
{ 
    return a*a; 
}

int main()
{
    hpx::future<int> f1 = hpx::async(square,10); 
    
    std::cout << f1.get() << std::endl;
    
    return EXIT_SUCCESS;
}

\end{lstlisting}

\begin{exercise}
Write the program in Listing~\ref{code:async:taylor} using \cpp{hpx::future} and \cpp{hpx::async}.
\end{exercise}

The benefit of using HPX is that more features for the synchronization of future is provided. In Listing~\ref{code:hpx:future:sync} some of these functionality is shown. In Line~1 a \cpp{std::vector} holding the \cpp{hpx::future<int>} is declared. In Lines~2--3 two futures of the two asynchronous function class are pushed to the vector. In Line~6 the expression \cpp{hpx::when_all} is used to make a barrier which waits until all computations of the asynchronous launched functions are ready. By calling \cpp{.then()} we specify what is done if all futures are ready. To do so, we provide a lambda function, see Section~\ref{sec:lambda:function}, which has a future with the \cpp{std::vector} of futures as its argument. In Line~7 we use the function \cpp{.get()} and this future to get the \cpp{std::vector} of futures. In line~7 and Line~8, we print the results as usual. Following synchronization options\link{https://stellar-group.github.io/hpx/docs/sphinx/latest/html/terminology.html\#term-lco} are available:
\begin{itemize}
\item \cpp{hpx::when_all} \\
It \textit{AND}-composes all the given futures and returns a new future containing all the given futures.
\item \cpp{hpx::when_any} \\
It \textit{OR}-composes all the given futures and returns a new future containing all the given futures.
\item \cpp{hpx::when_each} \\
It \textit{AND}-composes all the given futures and returns a new future containing all futures being ready.
\item \cpp{hpx::when_some} \\
It \textit{AND}-composes all the given futures and returns a new future object representing the same list of futures after n of them finished.
\end{itemize}



\begin{lstlisting}[language=c++,caption={Advanced synchronization of futures using HPX.\label{code:hpx:future:sync}},float,floatplacement=tb]
std::vector<hpx::future<int>> futures;

futures.push_back(hpx::async(square,10);
futures.push_back(hpx::async(square,100);

hpx::when_all(futures).then([](auto&& f){
 auto futures = f.get();
 std::cout << futures[0].get() 
 	<< " and " << futures[1].get();
});
\end{lstlisting}

\newpage
\theendnotes

%----------------------------------------------------------------------------------------
\part{Appendix}
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
\chapter*{Jupyter notebooks and GitHub classroom}
\addcontentsline{toc}{chapter}{\textcolor{azure}{Jupyter notebooks and GitHub classroom}}
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
\section*{Jupyter notebooks}
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
\section*{GitHub classroom}
%----------------------------------------------------------------------------------------

We use GitHub classroom\link{https://classroom.github.com} to submit the assignments. In this section, we go through the steps to submit the code to GitHub using git\link{https://git-scm.com/}. For a brief overview of the most common git commands, we refer to this cheat sheet\link{https://education.github.com/git-cheat-sheet-education.pdf} and for more details to~\cite{silverman2013git,laster2016professional}. The first step to submit the assignments is to get your GitHub\link{https://github.com/} account. We recommend to use a user name reflecting your name. If you want to use your local computer to submit the assignments, you have to install git on your computer to follow the following instructions. Note that git is installed on the course's web server, so we recommend to submit from there. Open a terminal on the course's web server and type \bash{git config --global user.name Surname Name} to set your Surname and Name, so one can see who submitted the assignment. Optional you can set your e-mail address using \bash{git config --global user.email you@provider.com}. \\


If you do not want to enter your password every time to use git, you can generate a ssh key\link{https://www.ssh.com/ssh/key/} as shown in Listing~\ref{git:ssh:key}. We use the command \bash{ssh-keygen} to generate the public and private key. It is common practice that the ssh-key is related to your e-mail address. We save the private key as \texttt{~/.ssh/id-rsa-github} and the public key as \texttt{~/.ssh/id-rsa-github.pub}. To avoid entering the password each time we do a commit to the assignment, we type \bash{ssh-add ~/.ssh/id-rsa-github} to add the key to our key ring. Note that you have to add the content of your public key to GitHub by clicking on Profile -> SSH keys and GPG keys -> New SSH key. \\

\begin{lstlisting}[language=bash,caption={Setting up a ssh key\label{git:ssh:key}},float,floatplacement=tb]
ssh-keygen -t rsa -C "you@provider.com"
Generating public/private rsa key pair.
Enter file in which to save the key (/home/diehlpk/.ssh/id-rsa): ~/.ssh/id-rsa-github
ssh-add ~/.ssh/id-rsa-github
\end{lstlisting}


For each assignment, you will get an e-mail and should click on the link there, see Figure~\ref{fig::github:invitation}, and accept the assignment, see Figure~\ref{fig::github:assignment}. After accepting the assignment you see a link which will be used to submit your assignment. Listing~\ref{git:push} shows how to submit your code to this assignment. Note that you will get a new invitation for each assignment. First, you use \bash{git clone} to clone the repository and after that you change the directory using the command \bash{cd}. For each file, you like to submit you run the command \bash{git add}. Note that you have to do this only once. Using the command \bash{git commit -a} you commit all files and with the command \bash{git push} you send them to the server, so the instructor can see and grade them.


\begin{figure}[bt]
\centering
\begin{subfigure}{.75\textwidth}
  \centering
\includegraphics[width=\textwidth]{images/2019-05-17-github-assingment}
\caption{Invitation for the assignment on GitHub.}
\label{fig::github:invitation}
\end{subfigure}

\begin{subfigure}{.75\textwidth}
  \centering
\includegraphics[width=\textwidth]{images/2019-05-17-github-assingment}
\caption{Confirmation of the acceptance and the link to submit your assignment.}
\label{fig::github:assignment}
\end{subfigure}
\caption{Accepting assignments on GitHub classroom.}
\end{figure}

\begin{lstlisting}[language=bash,caption={Setting up a ssh key\label{git:push}},float,floatplacement=tb]
git clone https://github.com/diehlpkteaching/test-diehlpk.git
cd test-diehlpk
touch exercise.cpp
git add exercise.cpp
# Work on your exercise
git commit -a
git push
\end{lstlisting}

\newpage
\theendnotes


%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\chapter*{Bibliography}
\addcontentsline{toc}{chapter}{\textcolor{azure}{Bibliography}} % Add a Bibliography heading to the table of contents

%------------------------------------------------

\section*{Articles}
\addcontentsline{toc}{section}{Articles}
\printbibliography[heading=bibempty,type=article]

%------------------------------------------------

\section*{Books}
\addcontentsline{toc}{section}{Books}
\printbibliography[heading=bibempty,type=book]




%----------------------------------------------------------------------------------------
%	INDEX
%----------------------------------------------------------------------------------------

\cleardoublepage % Make sure the index starts on an odd (right side) page
\phantomsection
\setlength{\columnsep}{0.75cm} % Space between the 2 columns of the index
\addcontentsline{toc}{chapter}{\textcolor{azure}{Index}} % Add an Index heading to the table of contents
\printindex % Output the index

%----------------------------------------------------------------------------------------
%	List of Listings
%----------------------------------------------------------------------------------------

\cleardoublepage % Make sure the index starts on an odd (right side) page
\phantomsection
\addcontentsline{toc}{chapter}{\textcolor{azure}{List of Listings}}
\lstlistoflistings

%----------------------------------------------------------------------------------------
%	List of Figures
%----------------------------------------------------------------------------------------

\cleardoublepage % Make sure the index starts on an odd (right side) page
\phantomsection
\addcontentsline{toc}{chapter}{\textcolor{azure}{List of Figures}}
\listoffigures

%----------------------------------------------------------------------------------------
%	List of Video lectures
%----------------------------------------------------------------------------------------

\chapter*{List of video lectures}
\addcontentsline{toc}{chapter}{\textcolor{azure}{Video lectures}}


\begin{itemize}
\item \href{https://www.youtube.com/watch?v=asGZTCR53KY&list=PL7vEgTL3FalY2eBxud1wsfz8OKvE9sd_z}{C++ Lecture 1 - The Standard Template Library}
\item \href{https://www.youtube.com/watch?v=iU3wsiJ5mts}{C++ Lecture 2 - Template Programming }
\item \href{https://www.youtube.com/watch?v=6PWUByLZO0g}{C++ Lecture 4 - Template Meta Programming}
\item \href{https://www.youtube.com/watch?v=Vck6kzWjY88}{The C++17 Parallel Algorithms Library and
Beyond}
\item \href{https://youtu.be/5vr7ItjyIH8}{Modern CUDA and C++}
\item \href{https://www.youtube.com/watch?v=Vck6kzWjY88}{The C++17 Parallel Algorithms Library and
Beyond}
\item \href{https://www.youtube.com/watch?v=js-e8xAMd1s}{The Asynchronous C++ Parallel Programming Model}
\item \href{https://www.youtube.com/watch?v=_Uv-_K3KTek&t=32s}{Nonlocality in Peridynamics}
\end{itemize}

\end{document}
