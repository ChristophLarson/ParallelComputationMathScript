%Book
%Copyright (C) 2019  Patrick Diehl
%
%This program is free software: you can redistribute it and/or modify
%it under the terms of the GNU General Public License as published by
%the Free Software Foundation, either version 3 of the License, or
%(at your option) any later version.

%This program is distributed in the hope that it will be useful,
%but WITHOUT ANY WARRANTY; without even the implied warranty of
%MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
%GNU General Public License for more details.

%You should have received a copy of the GNU General Public License
%along with this program.  If not, see <http://www.gnu.org/licenses/>.

% The template of the book is based on the The Legrand Orange Book
% LaTeX Template
% Version 2.4 (26/09/2018)
% Original author:
% Mathias Legrand (legrand.mathias@gmail.com) with modifications by:
% Vel (vel@latextemplates.com)
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt,fleqn]{book} % Default font size and left-justified equations

\input{structure.tex} % Insert the commands.tex file which contains the majority of the structure behind the template

\hypersetup{pdftitle={Parallel Computational Mathematics},pdfauthor={Patrick Diehl, Phd}} % Uncomment and fill out to include PDF metadata for the author and title of the book

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begingroup
\thispagestyle{empty} % Suppress headers and footers on the title page
\begin{tikzpicture}[remember picture,overlay]
%\node[inner sep=0pt] (background) at (current page.center) {\includegraphics[width=\paperwidth]{background.pdf}};
\draw (current page.center) node [fill=azure!30!white,fill opacity=0.6,text opacity=1,inner sep=1cm]{\Huge\centering\bfseries\sffamily\parbox[c][][t]{\paperwidth}{\centering Parallel Computaitonal Mathematics\\[15pt] % Book title
{\Large Fall 2020}\\[20pt] % Subtitle
{\huge Dr. Patrick Diehl}}}; % Author name
\end{tikzpicture}
\vfill
\endgroup

%----------------------------------------------------------------------------------------
%	COPYRIGHT PAGE
%----------------------------------------------------------------------------------------

\newpage
~\vfill
\thispagestyle{empty}

\noindent Copyright \copyright\ 2020 Patrick Diehl\orcid{0000-0003-3922-8419}\\ % Copyright notice

%\noindent \textsc{Published by Publisher}\\ % Publisher

\noindent \url{https://www.cct.lsu.edu/~pdiehl/teaching/2020/4997/}\\ % URL

\noindent \doclicenseThis  

\noindent \textit{Edition, Fall 2020 \tiny{(git commit \input{{.git/refs/heads/master}}})} % Printing/edition date

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

%\usechapterimagefalse % If you don't want to include a chapter image, use this to toggle images off - it can be enabled later with \usechapterimagetrue

%\chapterimage{chapter_head_1.pdf} % Table of contents heading image

\pagestyle{empty} % Disable headers and footers for the following pages

\tableofcontents % Print the table of contents itself

\cleardoublepage % Forces the first chapter to start on an odd page so it's on the right side of the book

\pagestyle{fancy} % Enable headers and footers again

\chapter*{Forward}

%----------------------------------------------------------------------------------------
%	PART
%----------------------------------------------------------------------------------------

\part{Introduction: C++ and the C++ standard template library}

%----------------------------------------------------------------------------------------
%	CHAPTER 1
%----------------------------------------------------------------------------------------

\include{sections/chapter1}

%----------------------------------------------------------------------------------------
\part{Linear algebra and solvers}
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
\chapter{Linear algebra}
%----------------------------------------------------------------------------------------
For the topic of linear algebra, we refer to~\cite{hefferonlinear,scheick1997linear}. We focus on vector and matrices and some operations on them, as we need them for example in the finite element method. Note that we look from the computer science perspective on matrices and vectors and focus how efficiently use existing libraries in our code. Several highly optimized C++ linear algebra libraries~\cite{wang2013augem,eigenweb,rupp2016viennacl,sanderson2016armadillo} are available. However, the look into the Blaze library since this library has a HPX backend for parallel computations. 


%----------------------------------------------------------------------------------------
\section{Blaze library}
%----------------------------------------------------------------------------------------
Blaze is an open-source, high-performance C++ math library for dense and sparse arithmetic. With its state-of-the-art Smart Expression Template implementation Blaze combines the elegance and ease of use of a domain-specific language with HPC-grade performance, making it one of the most intuitive and fastest C++ math libraries available. More details about the implementation details~\cite{doi:10.1137/110830125,6266939}.

%----------------------------------------------------------------------------------------
\subsection{Vectors}
\index{Blaze!vector}
\index{Blaze!vector}
\label{sec:linalg:vectors}
%----------------------------------------------------------------------------------------
A $n$ dimensional vector space (or linear space) $\mathbf{u}$ is defined as $\mathbf{u}=(u_1,u_2,\ldots,u_{n-1},u_{n-2})\in \mathbb{R}^n$. In Blaze, a three dimensional vector\link{https://bitbucket.org/blaze-lib/blaze/wiki/Vectors} is defined as \cpp{blaze::DynamicVector<int> c (3UL);}\link{https://bitbucket.org/blaze-lib/blaze/wiki/Vector\%20Types\#!dynamicvector}. Note the Blaze is a template based library as the STL and we have to provide the data type of the vector in the parenthesizes \cpp{<int>} and in the second parenthesizes \cpp{(3UL)} the size of the vector. as for the \cpp{std::vector} we can get the size of the vector by the expression \cpp{c.size()} and to access a value, we use \cpp{auto value = c[i];}. Listing~\ref{code:blaze:vector:iterator} shows how to iterate over a Blaze vector using the access operator \cpp{c[i]} and iterators \cpp{it-value}. For more details on iterators we refer to Section~\ref{ref:stl:iterators}. \\

\begin{lstlisting}[language=c++,caption={Iterating over a Blaze vector using a for loop with iterators.\label{code:blaze:vector:iterator}},float,floatplacement=tb]
#include <blaze/Math.h>

int main()
{
blaze::StaticVector<int,3UL> c{ 4, -2, 5 };

// Loop over the vector
for( size_t i=0UL; i< c.size(); ++i )
   std::cout << c[i] << std::endl;
   
// Iterate over a vector
blaze::CompressedVector<int> d{ 0, 2, 0, 0};
for( CompressedVector<int>::Iterator it=d.begin(); 
  it!=d.end(); ++it ) 
    std::cout << it->value() << std::endl;

}
\end{lstlisting}

For the three dimensional vector space, we look into so some common operations which are often needed in simulations, \emph{e.g.}\ in $N$-body simulation (Section~\ref{sec:nbody}) or peridyanmic simulation (Section~\ref{sec:pd}). For a vector $\mathbf{u}=(x,y,z)\in\mathbb{R}^3$ the norm\index{vector!norm} or length of the vector reads as
\begin{align}
\vert\mathbf{u}\vert = \sqrt{x^2+y^2+z^2}
\end{align}
and its direction is given as $\sfrac{\mathbf{u}}{\vert\mathbf{u}\vert}$. The norm of the vector $\vert c \vert$ is computed in Blaze using the expression \cpp{const double norm = norm( b );}\link{https://bitbucket.org/blaze-lib/blaze/wiki/Vector\%20Operations\#!norms}. The inner product\index{vector!inner product} $\bullet$ reads as
\begin{align}
\mathbf{u}_1 \bullet \mathbf{u}_2 = x_1x_2 + y_1y_2 + z_1z_2\text{.}
\end{align}
Figure~\ref{fig:vector:inner} shows the angle $\Theta$ between the two vectors $\mathbf{u}_1$ and $\mathbf{u}_2$ defined using the inner product $\bullet$.  The inner product $\mathbf{u}_1 \bullet \mathbf{u}_2 $ is computed in Blaze using the expression \cpp{int result2 = inner(v1,v2);}\link{https://bitbucket.org/blaze-lib/blaze/wiki/Vector-Vector\%20Multiplication\#!inner-product-scalar-product-dot-product}. The cross product $\times$ is defined by
\begin{align}
\mathbf{u}_1 \times \mathbf{u}_2 = \vert\mathbf{u}_1 \vert \vert\mathbf{u}_2 \vert sin(\theta) \mathbf{n}
\end{align}
and its geometric interpretation is sketches in Figure~\ref{fig:vector:cross}. The cross product of the two vector is the orthogonal vector on the two vectors. In addition, the norm of the cross product $\vert\textcolor{azure}{\mathbf{u}_1}\times\textcolor{amaranth}{\mathbf{u}_2}\vert$ is the are spanned by the two vectors. A more accessible form is
\begin{align}
\left( \begin{matrix}
c_0 \\ c_1 \\ c_2
\end{matrix}\right) = \left( \begin{matrix}
y_2 z_2 - z_1 x_2 \\
z_1 x_2 - x_1 z_2 \\
x_1 y_2 - y_1x_2
\end{matrix}\right) \text{.}
\end{align}
The inner product $\mathbf{u}_1 \times \mathbf{u}_2 $ is computed in Blaze using the expression \cpp{cross(u1,u2);}.

\begin{figure}[tb]
\begin{subfigure}{.5\textwidth}
\center
\begin{tikzpicture}
\draw [->] (0,0) -- (2,2);
\draw [->] (0,0) -- (2,-2);
\draw [azure,thick,domain=-45:45] plot ({cos(\x)}, {sin(\x)});

\node at (3,0) {$\Theta=\arccos(\sfrac{\mathbf{u_1}\bullet\mathbf{u_2}}{\vert\mathbf{u_1}\vert\vert \mathbf{u_2}\vert})$};
\node[above] at (2,2) {$\mathbf{u}_1$};
\node[below] at (2,-2) {$\mathbf{u}_2$};
\end{tikzpicture}
\caption{The angle $\Theta$ between the two vectors $\mathbf{u}_1$ and $\mathbf{u}_2$ defined using the inner product $\bullet$. }
\label{fig:vector:inner}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\begin{tikzpicture}
\draw[->,azure,thick] (0,0) -- (2,0);
\draw[->] (0,0) -- (0,2);
\draw[->,amaranth,thick] (0,0) -- (1,1);
\draw (1,1) -- (3,1) -- (2,0);
\draw [black,domain=0:45] plot ({cos(\x)}, {sin(\x)});

\node[below,azure] at (1,0) {$\mathbf{u}_1$};
\node[left,amaranth] at (0.75,0.75) {$\mathbf{u}_2$};
\node[above] at (0,2) {$\textcolor{azure}{\mathbf{u}_1}\times\textcolor{amaranth}{\mathbf{u}_2}$};
\node[above] at (3,1) {$\vert\textcolor{azure}{\mathbf{u}_1}\times\textcolor{amaranth}{\mathbf{u}_2}\vert$};
\node at (0.5,0.25) {$\Theta$};
\end{tikzpicture}
\caption{Visualization of the inner product $\vert\textcolor{azure}{\mathbf{u}_1}\times\textcolor{amaranth}{\mathbf{u}_2}\vert$ which is the orthogonal vector on the two others. }
\label{fig:vector:cross}
\end{subfigure}
\caption{Geometric interpretation of the inner product $\bullet$ (\subref{fig:vector:inner}) and the cross product $\times$ (\subref{fig:vector:cross}).}
\end{figure}


%----------------------------------------------------------------------------------------
\subsection{Matrices}
\index{Blaze!matrix}
%----------------------------------------------------------------------------------------
A matrix $\mathbf{A}\in \mathbb{R}^{n,m}$ has $n$ rows and $m$ columns
\begin{align}
\mathbf{A} = \begin{pmatrix}
a_{1,1} & \ldots & a_{1,m} \\
\vdots & \ldots & \vdots \\
a_{n,1} & \ldots & a_{n,m} 
\end{pmatrix}
\end{align} 
and following matrix operations are defined:
\begin{itemize}
\item Scaling:
\begin{align}
 2 \mathbf{A} = \begin{pmatrix}
2a_{1,1} & \ldots & 2a_{1,m} \\
\vdots & \ldots & \vdots \\
2a_{n,1} & \ldots & 2a_{n,m} 
\end{pmatrix} 
\end{align}
\item Addition:
\begin{align}
\mathbf{A} + \mathbf{B}  = \begin{pmatrix}
a_{1,1} + b_{1,1}  & \ldots & a_{1,m} + b_{1,m} \\
\vdots & \ldots & \vdots \\
a_{n,1} + b_{n,1} & \ldots & a_{n,m} + b_{n,m} 
\end{pmatrix}
\end{align}
\item Matrix vector multiplication
\begin{align}
\mathbf{A}  \mathbf{v}  = \begin{pmatrix}
a_{1,1} * b_{1} + & \ldots & + a_{1,m} * b_{n} \\
\vdots & \ldots & \vdots \\
a_{n,1} * b_{1} + & \ldots & + a_{n,m} * b_{n} 
\end{pmatrix}\text{.}
\end{align}
\end{itemize}
let us look what kind of matrices are provided by Blaze and how to use them for calculations. Let us start with Blaze's matrix types\link{https://bitbucket.org/blaze-lib/blaze/wiki/Matrix\%20Types}, see Listing~\ref{code:blaze:matrix:types}. The first type is the \cpp{DynanmicMatrix<T>}\link{https://bitbucket.org/blaze-lib/blaze/wiki/Matrix\%20Types\#!dynamicmatrix} which is a arbitrary sized matrix with dynamically allocated elements of arbitrary type \cpp{T}. Note that Blaze is a template-based library and the template type is provided within the first braces. For more details for C++ templates, we refer to~\ref{sec:generic:programming}. In the second pair of braces, the dimension of the $n$ and $m$ are given. Note that the values are not initialized of this matrix which means that the values can have any value. For large matrices the \cpp{DynanmicMatrix<T>} matrix is the best option, especially if the dimensions are not known at compile time. If the matrix is small and the dimensions are known at compile time, a \cpp{blaze::StaticMatrix}\link{https://bitbucket.org/blaze-lib/blaze/wiki/Matrix\%20Types\#!staticmatrix} matrix should be used. In Line~7 we define the $3\times 4$ matrix, but do not allocate the memory yet and the matrix has zero rows and columns. Only after calling the constructor the memory is allocated. Note that the dimensions of the matrix are provided as template arguments in that case. All matrices are default row-major matrices and to switch to column-major matrices, the template argument \cpp{blaze::columnMajor} is available. The last matrix type is the \cpp{blaze::CompressedMatrix} which is used for sparse matrices\link{https://bitbucket.org/blaze-lib/blaze/wiki/Matrix\%20Types\#!sparse-matrices} with only few non-zero entries.\\

These are the main types of matrices provided by the Blaze library. However, there are some special purpose matrices which are often needed available. One is the identity matrix \cpp{blaze::IdentityMatrix} with has ones on all diagonal entries and is zero everywhere else. To have a matrix with zero valued elements, the \cpp{blaze::ZeroMatrix} is used.
 
\begin{lstlisting}[language=c++,caption={Blaze matrix types.\label{code:blaze:matrix:types}},float,floatplacement=tb]
// Definition of a 3x4 matrix 
// Values are not initialized
blaze::DynamicMatrix<int> A( 3UL, 4UL );

// Definition of a 3x4 matrix
// with 0 rows and columns
blaze::StaticMatrix<int,3UL,4UL> A;

// Definition of column-major matrix
// with 0 rows and columns
blaze::DynamicMatrix<double,blaze::columnMajor> C;

// Definition of a 3x4 integral row-major matrix
blaze::CompressedMatrix<int> A( 3UL, 4UL );

// Definition of a 3x3 identity matrix
blaze::IdentityMatrix<int> A( 3UL );

// Definition of a 3x5 zero matrix
blaze::ZeroMatrix<int> A( 3UL, 5UL );
\end{lstlisting}

For all matrices the size of the matrix \cpp{size( A )}; returns the total amount of elements $(n \times m)$. The number of rows are obtained by \cpp{M2.rows();} and the number of columns are obtained by \cpp{M2.columns();}. All matrix operations\link{https://bitbucket.org/blaze-lib/blaze/wiki/Matrix\%20Operations} are applied as \cpp{abs( A );} which means the absolute value of all matrix elements is computed. Note that the elements of a Blaze matrix are accessed using different kind of parentheses \cpp{A(0,0) = 1;} sets the first element of the matrix to one.\\

One often used task in linear algebra is decomposition\index{decomposition} of matrices\link{blaze}. Blaze implements following decomposition methods: Cholesky~\cite{cholesky2005resolution}, QR/RQ, and QL/LQ. Listing~\ref{code:blaze:decomposition} shows how to use LU~\cite{bunch1974triangular} decomposition method. Note that we do not cover this methods in this course, but it is an important feature you should know. For more details we refer for example to~\cite{press1992numerical}.

\begin{lstlisting}[language=c++,caption={Matrix decomposition methods in Blaze\label{code:blaze:decomposition}},float,floatplacement=tb]
blaze::DynamicMatrix<double,blaze::rowMajor> A;
// ... Resizing and initialization

blaze::DynamicMatrix<double,blaze::rowMajor> L, U, P;

// LU decomposition of a row-major matrix
lu( A, L, U, P );  

assert( A == L * U * P );
\end{lstlisting}

Another important feature is the computation of Eigenvalues\index{eigenvalue} and Eigenvectors\index{eingenvector} which is shown in Listing~\ref{code:blaze:eigen}.  Note that we do not cover this methods in this course, but it is an important feature you should know. For more details we refer for example to~\cite{press1992numerical}. 

\begin{lstlisting}[language=c++,caption={Matrix decomposition methods in Blaze\label{code:blaze:eigen}},float,floatplacement=tb]

// The symmetric matrix A
SymmetricMatrix< DynamicMatrix<double,rowMajor>> 
    A( 5UL, 5UL );  
// ... Initialization

// The vector for the real eigenvalues
DynamicVector<double,columnVector> w( 5UL );    
// The matrix for the left eigenvectors   
DynamicMatrix<double,rowMajor>     V( 5UL, 5UL );  

eigen( A, w, V );
\end{lstlisting}







%----------------------------------------------------------------------------------------
\subsubsection{Application}
%----------------------------------------------------------------------------------------
One application of matrices is communication between a group of people $P_1,\ldots,P_4$. Figure~\ref{fig:matrix:application:graph} shows the communication network of these four people as a directed graph. For example $P_1$ communications with $P_2$ and $P_4$. One question one can ask, is how long does it take to transfer a message from $P_3$ to $P_2$. To obtain this information, we can use a adjacency matrix~\cite{biggs1993algebraic} as in Equation~\eqref{eq:graph:matrix} where a matrix element $a_{1,2}=1$ means that there is an edge in the graph from $P_1$ to $P_2$. By doing this for all people in our group, we will get this matrix. This matrix will tell us that $P_1$ has contact with $P_2$ and $P_4$, $P_2$ with $P_3$ and so on.

\begin{align}
\mathbf{M} = \left\lbrace\begin{matrix}
0 & 1 & 0 & 1 \\
0 & 0 & 1 & 0 \\
1 & 0 & 0 & 1 \\
1 & 1 & 0 & 0
\end{matrix} \right\rbrace
\label{eq:graph:matrix}
\end{align}

To compute how knows the message after four cycles, we define
\begin{center}
$\mathbf{M}^4 = \mathbf{M} \cdot \mathbf{M} \cdot \mathbf{M} \cdot \mathbf{M} $,  
\end{center}
which means for $\mathbf{M}^n$, we have to do $n$ multiplications of $\mathbf{M}$. After the multiplications, we get following result
\begin{align*}
M^2 = \left\lbrace\begin{matrix}
1 &  1 & 1 &  0 \\
1 & 0 & 0  & 1 \\
1 & 2 & 0 & 1 \\
0 & 1 & 1 & 1
\end{matrix} \right\rbrace
\end{align*}
and see that person $P_3$ can send some message to Person $P_2$ in two cycles. For more applications, we refer to~\cite{scheick1997linear}. 

\begin{exercise}
Transfer the matrix in Equation~\ref{eq:graph:matrix} into a Blaze matrix and try to reproduce the resulting matrix by multiplying the matrix four times.
\end{exercise}

\begin{figure}[tb]
\centering
\begin{tikzpicture}
  \SetGraphUnit{3}
  \Vertex{P1}
  \WE(P1){P2}
  \WE(P2){P3}
  \WE(P3){P4}
  %
  \Edge(P1)(P2)
  \Edge(P1)(P4)
  \Edge(P2)(P3)
  \Edge(P3)(P1)
  \Edge(P3)(P4)
  \Edge(P4)(P1)
  \Edge(P4)(P2)
\end{tikzpicture}
\caption{Graph of the communication network.}
\label{fig:matrix:application:graph}
\end{figure}

%----------------------------------------------------------------------------------------
\section{Compiling code using Blaze}
%----------------------------------------------------------------------------------------
To use Blaze, we have to first install the library on our system\link{https://bitbucket.org/blaze-lib/blaze/wiki/Configuration\%20and\%20Installation}. Listing~\ref{code:blaze:inst:cmake} shows how to install Blaze using CMake and Listing~\ref{code:blaze:inst:manually} how to install Blaze manually. Note that you should check if there is a newer version of Blaze available.

\begin{minipage}{\linewidth}
\begin{minipage}{0.45\linewidth}
\begin{lstlisting}[language=bash,caption={Installing Blaze using CMake.\label{code:blaze:inst:cmake}}]
tar -xvf blaze-3.6.tar.gz
cd blaze-3.6
cmake -DCMAKE_INSTALL_PREFIX=/home/patrick/blaze .
make install
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{0.45\linewidth}
\begin{lstlisting}[language=bash,,caption={Installing Blaze manually.\label{code:blaze:inst:manually}}]
tar -xvf blaze-3.6.tar.gz
cd blaze-3.6
cp -r ./blaze /home/patrick/blaze
\end{lstlisting}
\end{minipage}
\end{minipage}

After installing Blaze, we can use preferable CMake, see Listing~\ref{code:blaze:compile:cmake}, or compile the code by hand, see Listing~\ref{code:blaze:compile:manually}. For more details about CMake, we refer to Section~\ref{sec:cmake}. Note that we have already installed Blaze on the server and there is no need to install Blaze on your own device.

\begin{minipage}{\linewidth}
\begin{minipage}{0.45\linewidth}
\begin{lstlisting}[language=bash,caption={Compilation using CMake.\label{code:blaze:compile:cmake}}]
find_package( blaze )
if( blaze_FOUND )
   add_library( blaze_target INTERFACE )
   target_link_libraries( blaze_target 
   	INTERFACE blaze::blaze )
endif()
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{0.45\linewidth}
\begin{lstlisting}[language=bash,,caption={Manually compilation.\label{code:blaze:compile:manually}}]
g++ -I/home/diehlpk/blaze BlazeTest.cpp
\end{lstlisting}
\end{minipage}
\end{minipage}

Currently, we only have compiled Blaze for serial execution. To compile Blaze with C++ 11 threads, we have to add following arguments \bash{-std=c++11 -DBLAZE_USE_CPP_THREADS} to the compiler and export following environment variable \bash{export BLAZE_NUM_THREADS=4  // Unix systems}. For HPX parallelism, we have to add following arguments \bash{-DBLAZE_USE_HPX_THREADS} to the compiler and run \bash{./a.out --hpx:threads=4} to use four threads. Fore more details, we refer to~\link{https://bitbucket.org/blaze-lib/blaze/wiki/Shared\%20Memory\%20Parallelization}.

%----------------------------------------------------------------------------------------
\chapter{Solvers}
%----------------------------------------------------------------------------------------
Another important task in applied mathematics is to solve linear equations systems. Before we dig into the numerical and implementation details, we look into one example we know from our school lessons in mathematics. Figure~\ref{fig:example:interesction} plots the two functions $f_1(x_1)=-\sfrac{3}{2}x_1+1$ and  $f_2(x_1)=-\sfrac{2}{6}x_1-\sfrac{8}{6}$. From a visual perspective one can see that the intersection of these two functions is at $(2,-2)$. However, for more complex functions or more degree of freedoms the visual approach can get cumbersome. Another approach is to formulate the corresponding linear equations systems and solve it to get the intersections. For the linear equation system, we want to have the both functions in the form $3x_1+2x_2=2$ and $2X_1+6x_2=-8$ which are just a different way to write $f_1(x_1)$ and $f_2(x_1)$. Now we want to define a matrix $\mathbf{M}$ and the right-hand side $\mathbf{b}$ to find the solution $\mathbf{x}$ as $\mathbf{M}\mathbf{x}=\mathbf{b}$. Using the second form the function representation, we get 
\begin{align}
\mathbf{M}\mathbf{x}&=\mathbf{b} \\
\left(\begin{matrix}
3 & 2 \\
2 & 6 \\
\end{matrix}\right)
\left(\begin{matrix}
x_1 \\
x_2
\end{matrix}\right)
&=\left(
\begin{matrix}
2 \\
-8
\end{matrix}\right)\text{.}
\end{align}
We know from school how to solve the matrix using Gaussian elimination~\cite{brown1999contextual}\index{Gaussian elimination}. In Equation~\eqref{eq:gauss:1} the first line is multiplied by two and the second line by three to get the same factor in the first column. In Equation~\eqref{eq:gauss:2} we can subtract the first line from the second line to get a zero in the second line. In Equation~\eqref{eq:gauss:3} we now can get the value for $x_2$ because we know $2x_2=-28\rightarrow x_2=-2$. In Equation~\eqref{eq:gauss:4} the first line is multiplied by seven and the second line by two to get the same factor in the second column. In Equation~\eqref{eq:gauss:5} the second row is subtracted from the first one. In Equation~\eqref{eq:gauss:6} we now can get the value for $x_1$ because we know $42x_1=84\rightarrow x_2=2$. Which is the same solution as visual obtained in Figure~\ref{fig:example:interesction}.\\

Note that one can implement the Gauss elimination, but the theoretical complexity of this algorithm is $\mathcal{O}(n^3)$ where $n$ is the number of unknowns. So this algorithm is feasible for thousands of unknown, but might not scale for millions of unknowns. In that case the so-called iterative methods are used. We will look into the Conjugate Gradien method in the next section. For more details about iterative methods\index{iterative methods} we refer to~\cite{olshanskii2014iterative}.


\begin{align}
\left(\begin{matrix}
3 & 2 \\
2 & 6 \\
\end{matrix}\right)
\left(\begin{matrix}
x_1 \\
x_2
\end{matrix}\right)
&=\left(
\begin{matrix}
2 \\
-8
\end{matrix}\right)
\quad \left|\begin{matrix}
\cdot 2 \\
\cdot 3
\end{matrix}\right. \label{eq:gauss:1}\\
\left(\begin{matrix}
6 & 4 \\
6 & 18 \\
\end{matrix}\right)
\left(\begin{matrix}
x_1 \\
x_2
\end{matrix}\right)
&=\left(
\begin{matrix}
4 \\
-24
\end{matrix}\right)
\quad\left|\begin{matrix}
 \\
- R1 \\
\end{matrix}\right. \label{eq:gauss:2} \\
\left(\begin{matrix}
6 & 4 \\
0 & 14 \\
\end{matrix}\right)
\left(\begin{matrix}
x_1 \\
x_2
\end{matrix}\right)
&=\left(
\begin{matrix}
4 \\
-28
\end{matrix}\right) 
\quad \left|\begin{matrix}
 \\
\rightarrow x_2 = -2
\end{matrix}\right. \label{eq:gauss:3} \\
\left(\begin{matrix}
6 & 4 \\
0 & 14 \\
\end{matrix}\right)
\left(\begin{matrix}
x_1 \\
x_2
\end{matrix}\right)
&=\left(
\begin{matrix}
4 \\
-28
\end{matrix}\right) 
\quad \left|\begin{matrix}
\cdot 7 \\
\cdot 2
\end{matrix}\right. \label{eq:gauss:4} \\
\left(\begin{matrix}
42 & 28 \\
0 & 28 \\
\end{matrix}\right)
\left(\begin{matrix}
x_1 \\
x_2
\end{matrix}\right)
&=\left(
\begin{matrix}
28 \\
-56
\end{matrix}\right) 
\quad \left|\begin{matrix}
-R2 \\
 \\
\end{matrix}\right. \label{eq:gauss:5} 
\\
\left(\begin{matrix}
42 & 0 \\
0 & 28 \\
\end{matrix}\right)
\left(\begin{matrix}
x_1 \\
x_2
\end{matrix}\right)
&=\left(
\begin{matrix}
42 \\
-56
\end{matrix}\right) 
\quad \left|\begin{matrix}
\rightarrow x_1 = 2 \\
\\
\end{matrix}\right. \label{eq:gauss:6} 
\end{align}

\begin{figure}[tb]
\centering
\begin{tikzpicture}
\begin{axis}[grid=both,ymin=-6,ymax=4,xmax=8,xmin=-4,
               minor tick num=1,axis lines = middle,xlabel=$x_1$,ylabel=$x_2$]
\addplot [domain=-4:6, samples=101]{-1.5*x+1} node[pos=0.425] (endofplotsquare) {};
\node [right] at (endofplotsquare) {$3x_1+2x_2=2$};
\addplot [domain=-4:6, samples=101]{-(1/3)*x-(8/6)} node[pos=0.675] (endofplotsquare2) {};
\node [right] at (endofplotsquare2) {$2x_1+6x_2=-8$};
\end{axis}
\end{tikzpicture}
\caption{Plots of the function $f_1$ and $f_2$ to visually obtain the intersection of the the two lines.}
\label{fig:example:interesction}
\end{figure}

%----------------------------------------------------------------------------------------
\section{Conjugate Gradient solver}
%----------------------------------------------------------------------------------------


\newpage
\theendnotes


%----------------------------------------------------------------------------------------
\part{Numerical examples}
\label{part:numerical:examples}
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
\chapter{Monte-Carlo methods}
\label{sec:monte:carlo}
\index{Monte Carlo Method}
%----------------------------------------------------------------------------------------
Monte Carlo methods are computational algorithms which rely on repeated random sampling to obtain numerical results. The principle is to use randomness to solve the problem because it is difficult or impossible to utilize other approaches. A fun fact is that when the Monte Carlo Method was developed in the 1940s by Ulam and von Neumann they called the method Monte Carlo which refers to the Monte Carlo Casino in Monaco where Ulam's uncle gambled. Today Monte Carlo methods are widely used in following three problem classes:
\begin{itemize}
\item Optimization,
\item Numerical integration, and
\item Probability distributions.
\end{itemize}
For the importance of the method we refer to~\cite{kroese2014monte} and for more details about Monte Carlo Methods, we refer to~\cite{shonkwiler2009explorations}.\\

Let us look into the computational aspects of the Monte Carlo methods. Independent on the problem class a general pattern is observed
\begin{enumerate}
\item Define the input parameters,
\item Randomly chose input parameters,
\item Do deterministic computations on the inputs,
\item Aggregate the results\text{.}
\end{enumerate}

\begin{figure}[h]
  \begin{center}
  \begin{tikzpicture}
\draw (0,0) -- (2,0) -- (2,2) -- (0,2) -- cycle;
\draw[fill=cadetgrey, opacity=0.5] (1,1) circle (1cm);
\draw[->] (1,1) -- (2,1);
\draw[fill=black] (1,1) circle (0.05cm);
\node[above] at (1.5,1) {\small $r=\sfrac{1}{2}$};
\draw[<->] (0,-0.35) -- (2,-0.35);
\node[below] at (1,-0.35) {\small $1$};
\draw[<->] (-0.35,0) -- (-0.35,2);
\node[left] at (-0.35,1) {\small $1$};
\end{tikzpicture}
  \end{center}
  \caption{Sketch of the geometry used within the Monte Carlo method to estimate the number $\pi$.}
  \label{fig:monte}
\end{figure}
To understand these for steps, we will compute the value of $\pi$ using a Monte Carlo method. Note that this example is just for educational purposes. Figure~\ref{fig:monte} sketches the two ingredient a unit square and a circle needed to estimate $\pi$. First, a unit square $1 \times 1$ which means both sides have the length of one is drawn. The area $A_s$ is one, since we have a unit square. Second, a circle with the radius of $r=\sfrac{1}{2}$ is drawn at the center of the unit square. The area of the circle is $A_c=\pi r^2$. Using the radius $r=\sfrac{1}{2}$ the area is $A_c=\pi(\sfrac{1}{2})^2=\sfrac{\pi}{4}$. Now, since we have defined the area of the circle and the square, we can use them to estimate the value of $\pi$ by
\begin{align}
A_c &= \sfrac{\pi}{4} \notag\\
\pi &= 4 A_c \notag\\
\pi &= 4 \sfrac{A_c}{A_s}\text{.}
\end{align}
Note that the going from the first equation to the second one is just a multiplication by four. Going from the second line to the third line, we use the fact that the area of the square is one. \\

Now, we can estimate $\pi$ by the general pattern described above
\begin{itemize}
\item \textbf{Define the input parameters}: \\ A coordinate  $(x,y)\in\mathbb{R}$ in the domain of the unit square $[0,1]\times [0,1]$
\item\textbf{ Randomly chose input parameters}:\\ We randomly draw values for $x$ and $y$ in the range of $[0,1]$ for $N$ times
\item \textbf{Do deterministic computations on the inputs}:  \\
we have to validate if the coordinate $(x,y)$ is inside the circle or not by do the computation $x^2+y^2\leq 1$. If the coordinate is inside the circle we increment $N_C$.
\item \textbf{Aggregate the results}: \\
We compute $\pi\approx \sfrac{4N_c}{N}$
\end{itemize}
\vspace{0.25cm}

Figure~\ref{fig:algorithm:monte} shows the flow chart of the algorithm for estimating $\pi$ using the Monte Carlo method. First, the decision if the current draw of the random number is less than the total amount of random numbers $N$. If we have not draw enough random numbers, we have to guess to two random numbers $x$ and $y$, see Section~\ref{sec:random:numbers} for how to generate random numbers in C++. Next, we have to check if the drawn coordinate $(x,y)$ is within the circle and update the number of points within the circle $N_c$. We have to repeat this tasks until $i>N$. If we have drawn enough random number, we can compute $\pi\approx\sfrac{4N_c}{N}$ and finish the program.\\

\begin{figure}[tb]
    \centering
	\begin{tikzpicture}[node distance=1.125cm, scale=0.75, transform shape]
	\node (start) [startstop] {Start}; 
	\node (dec1) [decision, below of=start , yshift=-1.cm] { $i < N$};
	\node (n0) [process, below of=dec1,yshift=-1.cm] {Draw random number $x$ and $y$};
	\node (dec2) [decision, below of=n0 , yshift=-1.cm] { $x^2 + y^2 < 1$};
	\node (n1) [process, below of=dec2,yshift=-1.cm] {Increment $N_C$};
	\node (n2) [process, below of=n1,yshift=-1.cm] {Increment $i$};
	\node (n2) [process, below of=n1,yshift=-1.cm] {Increment $i$};
	\node (n3) [process, right of=dec1,xshift=2.5cm] {Compute $\sfrac{4N_c}{N}$};
	\node (stop) [startstop, right of=dec1, xshift=6.cm] {Finished}; 
	%lines
	\draw [->] (start) -- (dec1);
	\draw [->] (dec1) -- (n0);
	\draw [->] (n0) -- (dec2);
	\draw [->] (n1) -- (n2);
	\draw [->] (n3) -- (stop);
	%dec1
	\draw [->] (dec1) -- node[anchor=north] {no} (n3);
	\draw [->] (dec1) -- node[anchor=west] {yes} (n0);	
	\draw [->] (dec2) -- node[anchor=west] {yes} (n1);	
	\draw [->] (n2.west) -- ++(-3.0,0) -- ++(0,8.5) -- ++(3.5,0);
	\draw [->] (dec2)  -- ++(-4.5,0) ;
	\end{tikzpicture}
	\caption{Flow chart for the Monte Carlo method to estimate $\pi$.}
	\label{fig:algorithm:monte}
\end{figure}

Next, we can ask the question what is a good choice for $N$ to get a good approximation of $pi$. Figure~\ref{fig:monte:carlo:samples} shows the distribution of the point inside the circle (\textcolor{amaranth}{red}) and outside of the circle (\textcolor{azure}{blue}) for $N=10$, $N=100$, and $N=1000$ random numbers. One can see that a certain amount of random numbers is needed to have enough samples inside and outside of the circle. Figure~\ref{fig:monte:carlo} shows the absolute error in percent for various amount of random numbers. One can see that with thousand random numbers the accuracy is quite reasonable. 


\begin{figure}[bt]
\begin{subfigure}{.3\textwidth}
  \centering
\def\x{0}
\def\y{0}
\def\k{0}
\def\radius{4}
\begin{tikzpicture}
    \draw[fill=cadetgrey, opacity=0.1] (\radius,0) arc(0:90:\radius) -- (0,0) -- cycle;
    \draw[gray, opacity=0.25] (0,0) rectangle (\radius,\radius);
    \draw[->] (0,0) -- (1.1*\radius,0);
    \draw[->] (0,0) -- (0,1.1*\radius);
    \foreach \i in {1,2,...,10}{%
        \pgfmathsetmacro\x{\radius*rnd}%
        \pgfmathsetmacro\y{\radius*rnd}%
        \pgfmathsetmacro\k{(pow(\x,2)+pow(\y,2)) <pow(\radius,2)}%
        \pgfmathparse{ifthenelse(\k==1,"amaranth","azure")}%
        \fill[\pgfmathresult] (\x,\y)circle(0.75pt);%
    }
\end{tikzpicture}
  \caption{$N=10$}
  \label{fig:sub-first}
\end{subfigure}
\hfill
\begin{subfigure}{.3\textwidth}
  \centering
\def\x{0}
\def\y{0}
\def\k{0}
\def\radius{4}
\begin{tikzpicture}
    \draw[fill=cadetgrey, opacity=0.1] (\radius,0) arc(0:90:\radius) -- (0,0) -- cycle;
    \draw[gray, opacity=0.25] (0,0) rectangle (\radius,\radius);
    \draw[->] (0,0) -- (1.1*\radius,0);
    \draw[->] (0,0) -- (0,1.1*\radius);
    \foreach \i in {1,2,...,100}{%
        \pgfmathsetmacro\x{\radius*rnd}%
        \pgfmathsetmacro\y{\radius*rnd}%
        \pgfmathsetmacro\k{(pow(\x,2)+pow(\y,2)) <pow(\radius,2)}%
        \pgfmathparse{ifthenelse(\k==1,"amaranth","azure")}%
        \fill[\pgfmathresult] (\x,\y)circle(0.75pt);%
    }
\end{tikzpicture}
  \caption{$N=100$}
\end{subfigure}
\hfill
\begin{subfigure}{.3\textwidth}
  \centering
\def\x{0}
\def\y{0}
\def\k{0}
\def\radius{4}
\begin{tikzpicture}
    \draw[fill=cadetgrey, opacity=0.1] (\radius,0) arc(0:90:\radius) -- (0,0) -- cycle;
    \draw[gray, opacity=0.25] (0,0) rectangle (\radius,\radius);
    \draw[->] (0,0) -- (1.1*\radius,0);
    \draw[->] (0,0) -- (0,1.1*\radius);
    \foreach \i in {1,2,...,1000}{%
        \pgfmathsetmacro\x{\radius*rnd}%
        \pgfmathsetmacro\y{\radius*rnd}%
        \pgfmathsetmacro\k{(pow(\x,2)+pow(\y,2)) <pow(\radius,2)}%
        \pgfmathparse{ifthenelse(\k==1,"amaranth","azure")}%
        \fill[\pgfmathresult] (\x,\y)circle(0.75pt);%
    }
\end{tikzpicture}
  \caption{$N=1000$}
  \label{fig:sub-second}
\end{subfigure}
\caption{Distribution of the point inside the circle (\textcolor{amaranth}{red}) and outside of the circle (\textcolor{azure}{blue}) for $N=10$, $N=100$, and $N=1000$ random numbers.}
\label{fig:monte:carlo:samples}
% This example was adapted from https://tex.stackexchange.com/questions/244488/monte-carlo-method-drawing
\end{figure}


\begin{figure}[tb]
\centering
\begin{tikzpicture}
\begin{axis}
[
	xmin=0,   xmax=4,
	grid=major,
	xlabel=Numer of random numbers $10^x$,
	ylabel=Absolute Error (\%)
]
\montecarlo{10}{1}
\montecarlo{100}{2}
\montecarlo{1000}{3}
\end{axis}
\end{tikzpicture}
\caption{The absolute error in percent for various amount of random numbers. One can see that with thousand random numbers the accuracy is quite reasonable. }
\label{fig:monte:carlo}
\end{figure}

\begin{exercise}
Make a list of which C++ features we need to implement the flow chart in Figure~\ref{fig:algorithm:monte}.
\end{exercise}

\begin{exercise}
Implement the Algorithm in Figure~\ref{fig:algorithm:monte} using the random numbers in Section~\ref{sec:random:numbers}.
\end{exercise}


%----------------------------------------------------------------------------------------
\chapter{$N$-body problems}
\index{$N$-body problems}
\label{sec:nbody}
%----------------------------------------------------------------------------------------
The $N$-body problem is the physically problem of predicting the individual motions of a group of celestial objects interacting with each others gravitationally. In laymen terms we want to predict the interactive forces and the motion of all celestial bodies in all future times. We assume that the know their orbital properties, \emph{e.g.}\ the initial positions, velocity, and time.\\

Before we look into the $N$-body problem, we stepping back and look into the two-body problem. Let us look at two gravitational bodies with the massed $m_i$ and $m_j$ and their positions $\mathbf{r}_i,\mathbf{r}_j\in\mathbb{R}^3$. To define the equation of motion, we have to look in the following definitions:
\vspace{0.25cm}
\begin{enumerate}
\item \textbf{The Law of Gravitation}: \\
The force of $m_i$ acting on $m_j$ is 
\begin{align}
\mathbf{F}_{ij}= G m_i m_j \frac{\mathbf{r}_j-\mathbf{r}_2}{\vert \mathbf{r}_1-\mathbf{r}_i \vert^3}\text{,}
\end{align}
see Figure~\ref{fig:nbody:motion}. The universal constant of gravitation $G$ was estimated as $6.67408\cdot 10^{-11}m^3kg^{-1}s^{-2}$ in 2014~\cite{mohr2016codata}.
\item \textbf{Velocity and acceleration}: 
\begin{enumerate}
\item The velocity of $m_i$ reads as
\begin{align}
\mathbf{v}_i = \frac{d \mathbf{r}_i}{dt} \label{eq:nbody:vel}
\end{align}
\item The acceleration of $m_i$ reads as
\begin{align}
\mathbf{a}_i = \frac{d \mathbf{v}_i}{dt} \label{eq:nbody:acc}
\end{align}
\end{enumerate}
For more details about vector and basic vector operations, we refer to Section~\ref{sec:linalg:vectors}.
\item \textbf{The second Law of Mechanics}:  (Force is equal mass times acceleration) 
\begin{align}
\mathbf{F}= m \mathbf{a} \label{eq:law:first}
\end{align}
\end{enumerate}
With these three definitions, we can derive the equation of motion for the first body as follows
\begin{align}
\mathbf{F}_{ij}&=G m_i m_j \frac{\mathbf{r}_j-\mathbf{r}_i}{\vert \mathbf{r}_j-\mathbf{r}_i \vert^3} \label{eq:nbody:motion1} \\
m_i \mathbf{a}_i &= G m_i m_j \frac{\mathbf{r}_j-\mathbf{r}_i}{\vert \mathbf{r}_i-\mathbf{r}_j \vert^3} \label{eq:nbody:motion2} \\
\frac{d \mathbf{v}_i}{dt} & = G m_j \frac{\mathbf{r}_j-\mathbf{r}_i}{\vert \mathbf{r}_j-\mathbf{r}_i \vert^3} \label{eq:nbody:motion3} \\
\frac{d^2 \mathbf{r}_i}{dt^2} & = G m_j \frac{\mathbf{r}_j-\mathbf{r}_i}{\vert \mathbf{r}_j-\mathbf{r}_i \vert^3}
\label{eq:nbody:motion4}
\end{align}
To get from Equation~\eqref{eq:nbody:motion1} to Equation~\eqref{eq:nbody:motion2}, we substitute $\mathbf{F}_{ij}$ by $m_i \mathbf{a}_i$ using Equation~\ref{eq:law:first}.  From Equation~\eqref{eq:nbody:motion2} to Equation~\eqref{eq:nbody:motion3}, we divide by $m_i$ and substitute $\mathbf{a}_i$ by Equation~\ref{eq:nbody:acc}. From Equation~\eqref{eq:nbody:motion3} to From Equation~\eqref{eq:nbody:motion4}, we substitute Equation~\ref{eq:nbody:vel}. Note that we used Newton's law of universal gravitation~\cite{newton1833philosophiae}.\\


\begin{figure}[tb]
\centering
\begin{tikzpicture}
\draw (-2,0) circle (0.5cm);
\draw (2,0) circle (0.275cm);
\node at (-2,0) {$m_1$};
\node at (2,0) {$m_2$};
\draw[->] (-1.5,0)--(-1,0);
\draw[->] (1.7,0)--(1,0);
\node[below] at (-1,0) {$\mathbf{F}_1$};
\node[below] at (1,0) {$\mathbf{F}_2$};
\end{tikzpicture}
\caption{Sketch of the two celestial bodies with the masses $m_1$ and $m_2$ and the gravitational interaction forces $\mathbf{F}_1$ and $\mathbf{F}_1$. Equation~\ref{eq:nbody:motion4} shows the equation of motion for the two-body system.   }
\label{fig:nbody:motion}
\end{figure}

Now, we formulate the problem for $n$ bodies assuming that the force at one body is equal to the sum over all bodies, except the body itself
\begin{align}
\mathbf{F}_i = \sum\limits_{j=1,i\neq j}^n \mathbf{F}_{ij} = \sum\limits_{j=1,,i\neq j}^n G  m_j \frac{\mathbf{r}_j - \mathbf{r}_i}{\vert \mathbf{r}_j - \mathbf{r}_i\vert^3} \text{.} \label{eq:nbody:motion}
\end{align}
These are the laws of conservation for the $N$-body problem:
\begin{enumerate}
\item Linear Momentum: $\sum\limits_{i=1}^n m_i \mathbf{v}_i = M_0$
\item Center of Mass: $\sum\limits_{i=1}^n m_i \mathbf{r}_i = M_0 t + M_1$
\item Angular Momentum: $\sum\limits_{i=1}^n m_i (\mathbf{r}_i \times \mathbf{v}_i) = \mathbf{c}$
\item Energy: T-U=h with \\
$ T = \frac{1}{2} \sum\limits_{i=1}^n m_i \mathbf{v}_i \circ \mathbf{v}_i  , U= \sum\limits_{i=1}^n \sum\limits_{j=1}^n G \frac{m_i m_j}{\vert\mathbf{r}_i - \mathbf{r}_j\vert} $
\end{enumerate}
Note that these laws are just shown for completeness. For more details about the theory and the derivations, we refer to~\cite{aarseth2008cambridge,aarseth2003gravitational}. This book focuses on the implementation details and just provides the basics to implement the $N$-body problem within C++.

%----------------------------------------------------------------------------------------
\section{Algorithm}
%----------------------------------------------------------------------------------------
Figure~\ref{fig:nbody:algorithm} shows the three steps for the $N$-body simulation. In this section, we focus on the implementation details of the first two steps. Equation~\ref{eq:nbody:motion} shows how to compute the force for one celestial object. Recall, that the $\sum$ translate to a \cpp{for} loop as we discussed in Section~\ref{sec:iteration:statements}. To compute the forces of all bodies, the so-called nested \cpp{for} loop or direct sum is utilized. Listing~\ref{code:directsum} shows the concept of the direct sum which is robust, accurate, and completely general. However, the computational costs per body are $\mathcal{O}(n)$ and the computational costs for all bodies are $\mathcal{O}(n^2)$. The symbol $\mathcal{O}$ is the so-called Big O notation which is used for algorithms to describe how their run time or space space requirements grow as the input size grows. In our case the computational costs per body increase linear, since we have to compute the force $n-1$ times for all particles. The Big O notation $\mathcal{O}(n)$ means that the total amount of computational costs in less or equal $n$. These symbols are defined in the Bachmann–Landau notation\index{Bachmann–Landau notation}~\cite{bachmann1894analytische,landau2000handbuch,knuth1997art}. For all bodies the computational cost increases two the power of two since we have to compute the forces $n-1$ times for all all $n$ bodies. For small amount of celestial objects the direct sum is feasible, however, for larger amounts the tree-based codes or the Barnes-Hut method~\cite{barnes1986hierarchical} reduce the computational costs to $\mathcal{O}(n\log(n))$. \\

\begin{figure}[tb]
\centering
\begin{tikzpicture}
\draw (-3.75,0) rectangle (2,1) node[pos=.5] {Compute the forces};
\draw[->](-0.75,0) -- (-0.75,-1);
\draw (-3.75,-2) rectangle (2,-1) node[pos=.5] {Update the positions};
\draw[->](-0.75,-2) -- (-0.75,-3);
\draw (-3.75,-4) rectangle (2,-3) node[pos=.5] {Collect statiscal information};
\draw[->] (2,-3.5) -- (3,-3.5) -- (3,0.5) -- (2,0.5);
\end{tikzpicture}
\caption{The three steps of the algorithm for the $N$-body simulation. First, the forces for all objects are computed using Equation~\ref{eq:nbody:motion}. Second, the updated positions are computed using Equation~\ref{eq:position:update} and Equation~\ref{eq:position:update}. Third, the statistical information is evaluated.  }
\label{fig:nbody:algorithm}
\end{figure}


\begin{lstlisting}[language=c++,caption={Example for the so-called direct sum\index{direct sum}.\label{code:directsum}},float,floatplacement=tb]
for(size_t i = 0; i < bodies.size(); i++)
	for(size_t j = 0; j < bodies.size(); j++)
		if ( i != j )
			//Compute forces
\end{lstlisting}

For the second step of the algorithm, we need to update the positions for the evolution of the system over the time $T$. For the discretization in time, we define following quantities:
\begin{itemize}
\item $\Delta t$ the uniform time step size
\item $t_0$ the beginning of the evolution
\item $T$ the final time of the evolution
\item $k$ the time steps such that $k\Delta t=T$.
\end{itemize}
Next, we need to compute the derivatives to obtain the velocity and the acceleration of each celestial object. One numerical method to approximate the derivation is given by
\begin{align}
u'(x) \approx \frac{u(x+h)-u(x)}{h}
\end{align}
which is the s-called finite difference method. Figure~\ref{fig:nbody:finitedifference} sketches the principle of the finite difference method. For a sufficient small enough $h$, we can approximate the derivation at the coordinate $x$. For example choosing $h=1$ and $x=3$, we get $u'(x)=\sfrac{(4-3)}{1}=1$ which confirms with $u'(x)=1$ using the analytic derivation of $u(x)$. Now we can use Euler method to compute the updated positions at time $t_{k+1}$. First, we approximate the velocity using the finite difference scheme
\begin{align}
\mathbf{v}_i(t_k) = \frac{d\mathbf{r}_i}{dt} \approx \frac{\mathbf{r}_i(t_{k+1})-\mathbf{r}_i(t_k)}{\Delta t}\label{eq:vel}\text{.}
\end{align}
We do the same for the acceleration
\begin{align}
\mathbf{a}_i(t_k) = \frac{d\mathbf{v}_i}{dt}   \approx  \frac{\mathbf{v}_i(t_k)-\mathbf{v}_i(t_k-1)}{\Delta t} = \frac{\mathbf{F}_i}{m_i}  \label{eq:acc} 
\end{align}
from Equation~\ref{eq:law:first} we get $\mathbf{a}_i=\sfrac{\mathbf{F}_i}{m_i}$. More details~\cite{strikwerda2004finite,leveque2007finite,euler1824institutionum}. With the above approximations the velocity is computed as
\begin{align}
\mathbf{v}_i(t_k) = \mathbf{v}_i(t_{k-1}) + \Delta t \frac{\mathbf{F}_i}{m_i} \label{eq:position:update}
\end{align}
using Equation~\eqref{eq:acc} and the fact that the finite difference approximation of the acceleration is equal to $\sfrac{\mathbf{F}_i}{m_i}$. Finally, the updated position is computed as
\begin{align}
\mathbf{r}_i(t_{k+1}) = \mathbf{r}_{t_k} + \Delta t \mathbf{v}_i(t_k)  \label{eq:position:update}
\end{align} 
using Equation~\eqref{eq:vel}. Note that we used easy methods to update the positions and more sophisticated methods, \emph{e.g.}\ Crank--Nicolson method~\cite{crank1947practical}, are available

\begin{exercise}
Look into the equations in this section and try to derive the Equation~\ref{eq:position:update} and Equation~\ref{eq:position:update} on your own.
\end{exercise}

\begin{exercise}
Implement the $N$-body problem using the template code\link{https://github.com/diehlpkteaching/N-Body} on GitHub.
\end{exercise}

\begin{figure}[tb]
\centering
\begin{tikzpicture}
\begin{axis}[
        xmin=0, xmax=6, % x scale
        ymin=0, ymax=6, % y scale
]
\addplot[azure,thick]    {x};
\legend{$u(x)$}
\addplot[amaranth, mark=*] coordinates{(3,3)} node[midway,above left,amaranth] {$u(x)$};
\addplot[amaranth,  mark=*] coordinates{(4,4)} node[midway,above left,amaranth] {$u(x+h)$};
\addplot +[mark=none,cadetgrey] coordinates {(3, 3) (4, 3)};
\addplot +[mark=none,cadetgrey] coordinates {(4, 3) (4, 4)};
\addplot[cadetgrey,  mark=none] coordinates{(3.5,3)} node[midway,below] {$h$};
\end{axis}
\end{tikzpicture}
\caption{The principle of the finite difference method. For a sufficient small enough $h$, we can approximate the derivation at the coordinate $x$. For example choosing $h=1$ and $x=3$, we get $u'(x)=\sfrac{(4-3)}{1}=1$ which confirms with $u'(x)=1$ using the analytic derivation of $u(x)$. Now we can use Euler method to compute the updated positions at time $t_{k+1}$.}
\label{fig:nbody:finitedifference}
\end{figure}

%----------------------------------------------------------------------------------------
\chapter{Peridynamics}
\label{sec:pd}
%----------------------------------------------------------------------------------------


%----------------------------------------------------------------------------------------
\chapter{One-dimensional heat equation}
%----------------------------------------------------------------------------------------



\newpage
\theendnotes


%----------------------------------------------------------------------------------------
\part{Parallel and distributed computing}
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
\chapter{Parallel computing}
%----------------------------------------------------------------------------------------
In this Chapter, a brief overview of the technical aspects of parallel computing is given. Note that this course focuses on the implementation details, like asynchronous programming, see Chapter~\ref{sec:async:coding}; parallel algorithms, see Section~\ref{sec:stl:parallel:algorithms}; and the C++ standard library for parallelism and concurrency (HPX), see Chapter~\ref{sec:hpx}. However, we provide some details and further references for the technical aspects and hardware details.\\

Let us begin with a definition of parallelism: \textit{1)} we need multiple resources which can operate at the same, \textit{2)} we have to have more than one task that can be performed at the same time, \textit{3)} we have to do multiple tasks on multiple resources the same time. First, we have to have multiple resources, \emph{e.g.}\ multiple threads of a computation node at the same time. However, with current hardware architecture this is not an issue. Second, this part is more interesting, since we need some code which is independent of each other and can be executed concurrent. Third, here we want to have overlapping computations and communication on multiple resources. For more details about parallel computing, we refer to~\cite{grama2003introduction,trobec2018introduction}.\\

For the second part of the definition, Amdahl's law\index{Amdahl's law}~\cite{amdahl1967validity} or strong scaling\index{scaling} is important. Amdahl's law is given as
\begin{align}
S = \frac{1}{(1-P) + \frac{P}{N}}
\end{align}
where $S$ is the speed up, $P$ the proportion of parallel code, and $N$ the numbers of threads. Figure~\ref{fig:amdals:law} plots Amdahl's law for different ratios of parallel code. Obviously, for zero percent parallel code, there is no speedup. If the portion to parallel code increases, the speedup increases up to a certain amount of threads. Therefore, the parallel computing with many threads is only beneficial for highly parallelism in our program. For example if our code took 20 hours using a single thread to complete and there in a part of one hour which can not be executed in parallel. Thus, only 19 hours of execution time can be parallized $(p=0.95)$ and independent of the amount of threads we use the theoretical speedup is limited to $S=\sfrac{1}{(1-p)}=20$.\\

\begin{figure}[tb]
\centering
 \begin{tikzpicture}
        \begin{axis}[
            no markers,
            samples=100,
            /pgf/declare function={
            f(\x,\a) = 1. / ((1.-\a)+(\a/x));
       		},
       		grid=both,
       		xlabel=$N$ number of threads,
       		ylabel=$S$ speedup,
       		legend pos=outer north east,
        ]
            % ... and use it here
            \addplot+ [domain=1:2048,azure,thick] {f(x,0)};
            \addlegendentry{$P=0\%$}
            \addplot+ [domain=1:2048,cadetgrey,thick] {f(x,0.5)};
            \addlegendentry{$P=50\%$}
            \addplot+ [domain=1:2048,darkgreen,thick] {f(x,.75)};
            \addlegendentry{$P=75\%$}
            \addplot+ [domain=1:2048,amaranth,thick] {f(x,.90)};
            \addlegendentry{$P=90\%$}
            \addplot+ [domain=1:2048,black,thick] {f(x,.95)};
            \addlegendentry{$P=95\%$}
       \end{axis}
    \end{tikzpicture}
\caption{Plot of Amdahl's law for different parallel portions of the code.}
\label{fig:amdals:law}
\end{figure}

Before we look into different parallelism approaches, we look into the example how to compute the dot product $S = \mathbf{X} \cdot \mathbf{V} = \sum_i^N x_i y_i$ of two vectors $\mathbf{X} = \lbrace x_1,x_2,\ldots,x_n \rbrace$ and $\mathbf{Y} = \lbrace y_1,y_2,\ldots,y_n \rbrace$  in a sequential manner and extend this example to the various parallelism approaches. So we have to compute $S = (x_1y_1) + (x_2y_2) + \ldots + (x_n y_n)$ as shown in the flow chart in Figure~\ref{fig:flow:chart:dot:seq}. In the sequential processing, the first to elements of each vector are multiplied $x_1 \times y_1$ and added to the temporal result. After that the second elements are multiplied and added to the temporal result, and so on. \\

\begin{figure}
\centering
\begin{tikzpicture}
\draw (0,0) circle [radius=0.3] node {$\times$};
\draw (2,0) circle [radius=0.3] node {$\times$};
\draw (4,0) circle [radius=0.3] node {$\times$};
\draw (6,0) circle [radius=0.3] node {$\times$};
\draw (10,0) circle [radius=0.3] node {$\times$};
\node[above] at (8,0) {$\ldots$} ;

\draw (2,-1) circle [radius=0.3] node {$+$};
\draw (4,-1) circle [radius=0.3] node {$+$};
\draw (6,-1) circle [radius=0.3] node {$+$};
\draw (10,-1) circle [radius=0.3] node {$+$};
\node[above] at (8,-1) {$\ldots$} ;

\draw[->](0,-0.3) -- (1.7,-1);
\draw[->](2,-0.3) -- (2,-0.7);
\draw[->](4,-0.3) -- (4,-0.7);
\draw[->](6,-0.3) -- (6,-0.7);
\draw[->](10,-0.3) -- (10,-0.7);

\draw[->](2.3,-1) -- (3.7,-1.);
\draw[->](4.3,-1) -- (5.7,-1.);
\draw[->](6.3,-1) -- (7.7,-1.);
\draw[->](8.3,-1) -- (9.7,-1.);

\draw (-0.15,0.3) -- (-0.3,0.6);
\node[above] at (-0.3,0.6) {$x_1$} ;
\draw (2-0.15,0.3) -- (2-0.3,0.6);
\node[above] at (2-0.3,0.6) {$x_2$} ;
\draw (4-0.15,0.3) -- (4-0.3,0.6);
\node[above] at (4-0.3,0.6) {$x_3$} ;
\draw (6-0.15,0.3) -- (6-0.3,0.6);
\node[above] at (6-0.3,0.6) {$x_4$} ;
\draw (10-0.15,0.3) -- (10-0.3,0.6);
\node[above] at (10-0.3,0.6) {$x_n$} ;

\draw (0.15,0.3) -- (0.3,0.6);
\node[above] at (0.3,0.6) {$y_1$} ;
\draw (2+0.15,0.3) -- (2+0.3,0.6);
\node[above] at (2+0.3,0.6) {$y_2$} ;
\draw (4+0.15,0.3) -- (4+0.3,0.6);
\node[above] at (4+0.3,0.6) {$y_3$} ;
\draw (6+0.15,0.3) -- (6+0.3,0.6);
\node[above] at (6+0.3,0.6) {$y_4$} ;
\draw (10+0.15,0.3) -- (10+0.3,0.6);
\node[above] at (10+0.3,0.6) {$y_n$} ;

\draw[->](10,-1.3) -- (10,-1.6);
\node[below] at (10,-1.6) {$s$} ;
\end{tikzpicture}
\caption{Flow chart of the sequential evaluation of the dot product of two vectors. }
\label{fig:flow:chart:dot:seq}
\end{figure}

The first parallelism approach is the pipeline parallelism\index{pipeline parallelism}~\cite{ramamoorthy1977pipeline}. The pipeline parallelism is used in vector processors and in execution pipelines in all general microprocessors. Let us look into some example of from the automotive industry. First, the body of the car is assembled. Second, workers assemble the chassis. Third, workers add the engine into the chassis. Next, the steering wheel is added and many more steps until the car is finally assembled. TO make this process efficient, the workers assembling the chassis do not wait until the last step is finalized before they start working on the next chassis. Side note this is similar to the assembly line introduced bu Henry Ford to enable mass production of cars~\cite{watts2009people}.\\

Figure~\ref{fig:dataflow:pipeline} shows the data flow chart for the pipeline parallelism. In the first step, the values $x_1$ and $y_1$ are read from memory. In the second step the values are multiplied. In the last step the result of the multiplication is added to the variable $S$. However, the other threads do not idle until the result is computed and do a previous step if possible. Meaning if the multiplication at stage two is happening, another thread starts to get the next values. For more details, we refer to~\cite{quinn2003parallel}. \\


\begin{figure}[tb]
\centering
\begin{tikzpicture}
\draw (0,0) rectangle ++(0.75,0.75) node[pos=.5] {$+S$};
\draw (1.5,0) rectangle ++(0.75,0.75) node[pos=.5] {$xy$}; 
\draw (3.5,0) rectangle ++(2,0.75) node[pos=.5] {\text{get} $x_i$,$y_i$};

\node[above] at (8,0.5*0.75) {$\mathbf{X} = \lbrace x_1,x_2,\ldots,x_n \rbrace$} ;
\node[below] at (8,0.5*0.75) {$\mathbf{Y} = \lbrace y_1,y_2,\ldots,y_n \rbrace$} ;
\node at (-1,0.5*0.75) {$S$} ;

\draw[<-](-0.8,0.5*0.75) -- (0,0.5*0.75) ;
\draw[<-](0.75,0.5*0.75) -- (1.5,0.5*0.75) ;
\draw[<-](2.25,0.5*0.75) -- (3.5,0.5*0.75) ;
\draw[<-](5.75,0.8*0.75) -- (6,0.8*0.75) ;
\draw[<-](5.75,0.2*0.75) -- (6,0.2*0.75) ;
\end{tikzpicture}
\caption{Flow chart for the pipeline processing for the dot product.}
\label{fig:dataflow:pipeline}
\end{figure}

The second parallelism approach is the Single instructions and multiple data (SIMD)\index{SIMD}\index{single instructions and multiple data}. SIMD is part of Flynn's taxonomy, a classification of computer architectures, proposed by Michael J. Flynn in 1966~\cite{flynn1972some,duncan1990survey}. Following aspects are relevant 
\begin{itemize}
\item All perform same operation at the same time
\item But may perform different operations at different times
\item Each operates on separate data
\item Used in accelerators on microprocessors
\item Scales as long as data scales.
\end{itemize}
\vspace{0.25cm}
Figure~\ref{fig:reduction:tree:simd} shows the reduction tree for the dot product computation. For this parallelism approach all threads perform the same operation at the same time. In our case all available threads multiply two values at the first level. Second one of these threads add the partial results. Until not all elements are read from the vector these steps are repeated. The last step is to accumulate all partial results and the final result is available. For example previous CUDA architectures were designed this way and introducing branching had some effect on the performance. Newer CUDA architectures perform better here and these things are explained in following talk\link{https://youtu.be/5vr7ItjyIH8}.

\begin{figure}[tb]
\centering
  \begin{tikzpicture}
   \draw (0,0) rectangle ++(0.75,0.75) node[pos=.5] {$P_1$}; 
   \draw (1.5,0) rectangle ++(0.75,0.75) node[pos=.5] {$P_2$}; 
   \draw (3,0) rectangle ++(0.75,0.75) node[pos=.5] {$P_3$}; 
   \draw (4.5,0) rectangle ++(0.75,0.75) node[pos=.5] {$P_4$}; 
   
   \draw (0.75,-2) rectangle ++(0.75,0.75) node[pos=.5] {$+$}; 
   \draw (3.75,-2) rectangle ++(0.75,0.75) node[pos=.5] {$+$}; 
   
   \draw (2.25,-4) rectangle ++(0.75,0.75) node[pos=.5] {$+$}; 
   
   \draw[->] (0.75+0.5*0.75,-2) -- (2.25,-4+0.75) ;
   \draw[->] (3.75+0.5*0.75,-2) -- (3,-4+0.75) ;
   
   \draw[->] (1.5+0.5*0.75,0) -- (1.5,-2+0.75) ;
   \draw[->] (0.5*0.75,0) -- (0.75,-2+0.75) ; 
   
   \draw[->] (3+0.5*0.75,0) -- (3.75,-2+0.75) ;
   \draw[->] (4.5+0.5*0.75,0) -- (4.5,-2+0.75) ; 
   
   \draw[->] (2.25+0.5*0.75,-4) -- (2.25+0.5*0.75,-4.5) ; 
   
   \draw[dashed] (0,-0.75) -- (5,-0.75) ;
   \node[below,rotate=90] at (-.75,-2) {\small Reduction tree}; 
   
   \node at (0.5*0.75,1.5) {\tiny $X=\lbrace x_1,x_2 \rbrace$};
   \node at (0.5*0.75,1.25) {\tiny $Y=\lbrace x_9,x_{10} \rbrace$};
   \node at (1.5+0.5*0.75,1.5) {\tiny $X=\lbrace x_3,x_4 \rbrace$};
   \node at (1.5+0.5*0.75,1.25) {\tiny $Y=\lbrace x_{11},x_{12} \rbrace$};
   \node at (3.+0.5*0.75,1.5) {\tiny $X=\lbrace x_5,x_6 \rbrace$};
   \node at (3.+0.5*0.75,1.25) {\tiny $Y=\lbrace x_{13},x_{14} \rbrace$}; 
   \node at (4.5+0.5*0.75,1.5) {\tiny $X=\lbrace x_7,x_8 \rbrace$};
   \node at (4.5+0.5*0.75,1.25) {\tiny $Y=\lbrace x_{15},x_{16} \rbrace$};
   
   \draw[->] (0.5*0.75,1.125) -- (0.5*0.75,0.75) ;
   \draw[->] (1.5+0.5*0.75,1.125) -- (1.5+0.5*0.75,0.75) ;
   \draw[->] (3.+0.5*0.75,1.125) -- (3.+0.5*0.75,0.75) ;
   \draw[->] (4.5+0.5*0.75,1.125) -- (4.5+0.5*0.75,0.75) ;
   \end{tikzpicture}
   \caption{Reduction tree for the dot product using single instructions and multiple data.}
   \label{fig:reduction:tree:simd}
\end{figure}

%----------------------------------------------------------------------------------------
\subsection*{Memory access}
\index{memory access}
%----------------------------------------------------------------------------------------
For parallel computing, the memory access scheme is important to understand performance behavior. If we initialize for example the two vectors in the dot product example, some space in the memory is reserved and filled with the values. For the computation of the dot product these elements have to be read from memory and the CPU is doing the computation. In a layman's view the CPU is connected to the memory via a so-called bus. Depending on the bus's architecture the access time differs and may have effects on the performance if there is a switch from one CPU to the second CPU.\\

The first memory access scheme is uniform memory access (UMA), see Figure~ref{fig:memory:uma},  where all memory is attached to one bus and all CPU are attached to the same bus. Therefore, the memory access times are the same for all CPU. So we do not see any effect if we switch from one two two CPU. The second memory access scheme is non-uniform memory access (NUMA), see Figure~\ref{fig:memory:numa}. Here, the access time to the memory depends on the memory location relative to the CPU. Thus, local memory access is fast and non-local memory access has some overhead. For more details about memory access, we refer to~\cite{el2005advanced,hager2010introduction}.

\begin{figure}
\centering
\begin{minipage}[c]{0.45\textwidth}
\centering
   \begin{tikzpicture}
%Threads
\draw (0,3) rectangle (.5,3.5) node[pos=.5] {1};
\draw (0.5,3) rectangle (1.0,3.5) node[pos=.5] {..};
\draw (1.,3) rectangle (1.5,3.5) node[pos=.5] {n};

\draw (2.5,3) rectangle (3.,3.5) node[pos=.5] {1};
\draw (3.,3) rectangle (3.5,3.5) node[pos=.5] {..};
\draw (3.5,3) rectangle (4,3.5) node[pos=.5] {n};

\draw (0.75,2.5) -- (0.75,3.);
\draw (3.25,2.5) -- (3.25,3.);

%BUS
\draw (0,1) rectangle (4,1.5) node[pos=.5] {Bus};


%CPU
\draw (0,2) rectangle (1.5,2.5) node[pos=.5] {CPU 1};
\draw (2.5,2) rectangle (4,2.5) node[pos=.5] {CPU 2};
\draw (0.75,2) -- (0.75,1.5);
\draw (3.25,2) -- (3.25,1.5);

%Memory
\draw (0,0) rectangle (4,0.5) node[pos=.5] {Memory};
\draw (0.75,.5) -- (0.75,1.);
\draw (3.25,.5) -- (3.25,1.);
\end{tikzpicture}
    \caption{Uniform memory access (UMA)\index{uniform memory access}\index{UMA}}
    \label{fig:memory:uma}
\end{minipage}
\hfill
\begin{minipage}[c]{0.45\textwidth}
\begin{tikzpicture}
%Threads
\draw (0,3) rectangle (.5,3.5) node[pos=.5] {1};
\draw (0.5,3) rectangle (1.0,3.5) node[pos=.5] {..};
\draw (1.,3) rectangle (1.5,3.5) node[pos=.5] {n};

\draw (2.5,3) rectangle (3.,3.5) node[pos=.5] {1};
\draw (3.,3) rectangle (3.5,3.5) node[pos=.5] {..};
\draw (3.5,3) rectangle (4,3.5) node[pos=.5] {n};

\draw (0.75,2.5) -- (0.75,3.);
\draw (3.25,2.5) -- (3.25,3.);

%BUS
\draw (0,1) rectangle (1.5,1.5) node[pos=.5] {Bus};
\draw (2.5,1) rectangle (4,1.5) node[pos=.5] {Bus};

%CPU
\draw (0,2) rectangle (1.5,2.5) node[pos=.5] {CPU 1};
\draw (2.5,2) rectangle (4,2.5) node[pos=.5] {CPU 2};
\draw (0.75,2) -- (0.75,1.5);
\draw (3.25,2) -- (3.25,1.5);

%Memory
\draw (0,0) rectangle (1.5,0.5) node[pos=.5] {Memory};
\draw (2.5,0) rectangle (4,0.5) node[pos=.5] {Memory};
\draw (0.75,.5) -- (0.75,1.);
\draw (3.25,.5) -- (3.25,1.);
\end{tikzpicture}
    \caption{Non-uniform memory access (NUMA)\index{non-uniform memory access}\index{NUMA}}
    \label{fig:memory:numa}
\end{minipage}

\end{figure}



%----------------------------------------------------------------------------------------
\section{Caution: Data races and dead locks}
%----------------------------------------------------------------------------------------
Remember with great power comes great responsibility! Meaning with shared memory parallelism you add an additional source of error to your code. When using parallel execution policy, it is the programmer's responsibility to avoid
\begin{itemize}
\item data races
\item race conditions
\item deadlocks.
\end{itemize} 
Let us look into some code examples for these kind of errors. A data race\index{data race} exists when multithreaded (or otherwise parallel) code that would access a shared resource could do so in such a way as to cause unexpected results. Listing~\ref{code:data:race} shows an example for a data race for the variable \cpp{sum}. Since the parallel execution policy is used, multiple threads can access the variable \cpp{sum} at the same time which means that not all threads can write to the variable. Thus, the result is might not correct. There are two solutions to avoid the data race. First, the atomic library\link{https://en.cppreference.com/w/cpp/atomic/atomic}. The atomic library\footnote{\tiny\url{https://en.cppreference.com/w/cpp/atomic}} provides components for fine-grained atomic operations allowing for lockless concurrent programming. Each atomic operation is indivisible with regards to any other atomic operation that involves the same object. Atomic objects are free of data races. Listing~\ref{code:datarace:atomic} shows the solution using \cpp{std::atomic:<int>}\link{https://en.cppreference.com/w/cpp/atomic/atomic}. The second solution is shown in Listing~\ref{code:datarace:mutex}. Here, the \cpp{std::mutex} class is used to avoid the data race. The mutex class\link{https://en.cppreference.com/w/cpp/thread/mutex} is a synchronization primitive that can be used to protect shared data from being simultaneously accessed by multiple threads. In Line~4 a \cpp{std::mutex m;} is generated. In Line~8 the lock of the code is started by using \cpp{m.lock();} and in Line~10 the lock is released by using \cpp{m.unlock();}.

\begin{exercise}
Give a definition for \cpp{std::atomic} and \cpp{std::mutex} in your own words. 
\end{exercise}

Another source of error is the race condition\index{race condition} where a check of a shared variable within a parallel execution and another thread could change this variable before it is used. Listing~\ref{code:racecondition} shows the solution to avoid the race condition.  Imagine the code without the \cpp{std::mutex} and the implication to get a wrong result. In the code there is a check if \cpp{x} is equal to 5 and a special treatment of the computation in this case. Now in Line~4 it was true that \cpp{x} was equal to five and the thread enters the \cpp{if} branch. However, in between another thread could change the value of \cpp{x} and not \cpp{y = 5 *2} is computed. By using the mutex this situation is avoided.

\begin{exercise}
Explain a data race in your own words and explain why a \cpp{std::mutex} avoids the data race.
\end{exercise}

\begin{lstlisting}[language=c++,caption={Example code and Solution for a data race.\label{code:data:race}},float,floatplacement=tb]
//Compute the sum of the array a in parallel
int a[] = {0,1,2,3,4};
int sum = 0;
std::for_each(std::execution::par, 
              std::begin(a), 
              std::end(a), [&](int i) {
  sum += a[i]; // Error: Data race
});
\end{lstlisting}


\begin{lstlisting}[language=c++,caption={Solution to avoid the data race using \cpp{std::atomic}.\label{code:datarace:atomic}},float,floatplacement=tb]
//Compute the sum of the array a in parallel
int a[] = {0,1};
std::atomic<int> sum{0};
std::for_each(std::execution::par, 
              std::begin(a), 
              std::end(a), [&](int i) {
  sum += a[i]; 
});
\end{lstlisting}

\begin{lstlisting}[language=c++,caption={Solution to avoid the data race using \cpp{std::mutex}.\label{code:datarace:mutex}},float,floatplacement=tb]
//Compute the sum of the array a in parallel
int a[] = {0,1};
int sum = 0;
std::mutex m;
std::for_each(std::execution::par, 
              std::begin(a), 
              std::end(a), [&](int i) {
  m.lock();
  sum += a[i];
  m.unlock(); 
});
\end{lstlisting}


\begin{lstlisting}[language=c++,caption={Example for the race condition.\label{code:racecondition}},float,floatplacement=tb]
std::mutex m;

m.lock();
if (x == 5)  // Checking x
{
   // Different thread could change x 
      
   y = x * 2; // Using x
}
m.unlock();
// Now it is sure that y will be 10
\end{lstlisting}


A deadlock\index{deadlock} describes a situation where two or more threads are blocked forever and waiting for each others. Following example taken from\link{https://docs.oracle.com/javase/tutorial/essential/concurrency/deadlock.html} explains a deadlock nicely. \\

\textit{Alphonse and Gaston are friends, and great believers in courtesy. A strict rule of courtesy is that when you bow to a friend, you must remain bowed until your friend has a chance to return the bow. Unfortunately, this rule does not account for the possibility that two friends might bow to each other at the same time.}

\begin{exercise}
The implementation of this examples is available on GitHub\link{https://github.com/diehlpkteaching/ParallelComputationMathExamples/blob/master/chapter10/lecture6-deadlock.cpp.ipynb}. Play around with the example and try to understand why the code results in a deadlock. 
\end{exercise}

%----------------------------------------------------------------------------------------
\chapter{Asynchronous programming}
\label{sec:async:coding}
%----------------------------------------------------------------------------------------
A different concept for shared memory parallelism is asynchronous programming~\cite{williams2012c++}. Before we look into asynchronous programming, we look again into the concept of serial programming. Figure~\ref{fig:async:dependencygraph} shows the dependency graph for one computation and one can see that we can compute $P$ and $X$ independent and only $H$ depends on both of them. Listing~\ref{code:serial:dependency} shows the serial computation of the dependency graph. Each line of code is executed line by line Each time a function is called the code waits until the function finishes. Thus, we can not compute $P$ and $X$ independently, even if the data is independent. 

\vspace{0.25cm}
\begin{minipage}{\linewidth}
\begin{minipage}{0.45\linewidth}
\centering
\begin{tikzpicture}
\node (a) [draw,circle] at (0,2) {$H$};
\node (b) [draw,circle] at (0,0) {$P$};
\node (c) [draw,circle] at (1,0) {$X$};
\draw [->] (a) -- (b);
\draw [->] (a) -- (c);
\end{tikzpicture}
\captionof{figure}{Example dependency graph}
\label{fig:async:dependencygraph}
\end{minipage}
\hfill
\begin{minipage}{0.5\linewidth}
\begin{lstlisting}[language=c++,caption={Synchronous execution of the dependency graph.\label{code:serial:dependency}}]
auto P = compute();
auto X = compute();
auto H = compute(P,X);
\end{lstlisting}
\end{minipage}
\end{minipage}
\vspace{0.25cm}

To executed lines asynchronously the C++ language provides the \cpp{std::async}\link{https://en.cppreference.com/w/cpp/thread/async}\index{async}\index{C++!async} expression provided by the \cpp{#include <future>}. Listing~\ref{code:serial:dependency} shows the asynchronous implementation of the dependency graph in Figure~\ref{fig:async:dependencygraph}. Line~2 shows the usage of \cpp{std::async} for the function \cpp{compute}. The first argument is the name of the function or a lambda expression, see Section~\ref{sec:lambda:function}. Because we used \cpp{std::async} this line of code is executed in the background on a different thread and the next line of code is executed, even if the result of the computation is not ready yet. Therefore, \cpp{std::async} return a \cpp{std::future<int>}\link{https://en.cppreference.com/w/cpp/thread/future}\index{future}\index{C++!future} object provided by the \cpp{#include <future>} header which is a template and contains the return type of the function which is in this example the \cpp{int} data type. In Line~4, the computation of $X$ is started on another thread. Such that both computations happens at the same time. In Line~7--9 the results of the asynchronous function call are gathered, since these are needed to compute $H$. With the \cpp{.get()} function a barrier is introduced and the line of codes waits until the computation is ready. In our case, we can wait since we need the two results to compute the last one. Meaning that Line~8 is only executed if the computation in Line~2 has finished. Following synchronization features are available:
\begin{itemize}
\item \lstinline|.get()| returns the result of the functions and wait until the computation finished
\item \lstinline|.wait()|\link{https://en.cppreference.com/w/cpp/thread/future/wait} waits until the computation finished
\item \lstinline|.wait_for(std::chrono::seconds(1))|\link{https://en.cppreference.com/w/cpp/thread/future/wait_for} returns if it is not available for the specified timeout duration 
\item \lstinline|.wait_until(std::chrono::seconds(1))|\link{https://en.cppreference.com/w/cpp/thread/future/wait_until} waits for a result to become available. It blocks until specified timeout time has been reached or the result becomes available, whichever comes first. 
\end{itemize}

\begin{lstlisting}[language=c++,caption={Asynchronous execution of the dependency graph.\label{code:serial:dependency}},float,floatplacement=tb]
// Compute P
std::future<int> f1 = std::async(compute);
// Compute X
auto f2 = std::async(compute);

// Gather the results
int P = f1.get();
int X = f2.get();

// Compute the dependent result H
std::cout << compute(P,X) << std::endl;
\end{lstlisting}

\section*{Example}
Let us look into one example to show the parallelism using asynchronous programming for the Taylor series. The approximation of the $\sin$ function is given as
\begin{align}
\sin(x) = \sum\limits_{n=0}^N (-1)^{n-1} \frac{x^{2n}}{(2n)!} 
\label{eq:sin:taylor}
\end{align}
One approach to parallize the above function using two threads is:
\begin{enumerate}
\item Split $n$ into slices, e.g. 2 times $\sfrac{n}{2}$ for two threads
\item Start two times \lstinline|std::async| where each thread computes $\sfrac{n}{2}$
\item Use the two futures to synchronize the results
\item Combine the two futures to obtain the result.
\end{enumerate}
To distribute $n$ into slices, we need to write the sum in Equation~\eqref{eq:sin:taylor} as
\begin{align}
\sum\limits_{n=begin}^end (-1)^{n-1} \frac{x^{2n}}{(2n)!} \text{.}
\end{align}
Listing~\ref{code:async:taylor} shows how to implement the function to splice the computation of the Taylor series, see Line~5--14. In Line~18--19 the two splices $\sfrac{n}{2}$ are launched from $0$ up to $49$ on the first thread and from $50$ up to $99$ on the second thread. In Line~22 the result is gathered and finally the accumulated result is evaluated. For more details, we refer to following talk\link{https://www.youtube.com/watch?v=js-e8xAMd1s}.

To compile the code using asynchronous programming, we need to add \lstinline|-pthread| to our compiler to use the POSIX threads to launch the functions asynchronous (\lstinline|std::async|). More details about POSIX\index{POSIX} threads~\cite{butenhof1997programming,kleiman1996programming}.




\begin{lstlisting}[language=c++,caption={Asynchronous computation of the $\sin$ function using a Taylor series.\label{code:async:taylor}},float,floatplacement=tb]
#include <future>
#include <iostream>

// Function to compute portion of the Taylor series
double taylor(size_t begin, size_t end, 
double x,size_t n){
double res = 0;

	for( size_t i = begin ; i < end ; i++)
	{
	  res += pow(-1,i-1) * pow(x,2*n) / factorial(2*n);
	} 
return res;
}

int main(){
	// Asynchronous computation using two slices
	auto f1 = std::async(taylor,0,49,2,100); 
	auto f2 = std::async(taylor,50,99,2,100); 
	
	// Gather the result
	double result = f1.get() + f2.get();

	// Print the result
	std::cout << "sin(2)= " res << std::endl;

	return 0;
}
\end{lstlisting}


%----------------------------------------------------------------------------------------
\chapter{Distributed memory}
%----------------------------------------------------------------------------------------


\newpage
\theendnotes


%----------------------------------------------------------------------------------------
\part{HPX}
\label{sec:hpx}
%----------------------------------------------------------------------------------------

\section{Introduction to HPX}

\section{Parallel algorithms}

\section{Asynchronous programming}

\newpage
\theendnotes


%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\chapter*{Bibliography}
\addcontentsline{toc}{chapter}{\textcolor{azure}{Bibliography}} % Add a Bibliography heading to the table of contents

%------------------------------------------------

\section*{Articles}
\addcontentsline{toc}{section}{Articles}
\printbibliography[heading=bibempty,type=article]

%------------------------------------------------

\section*{Books}
\addcontentsline{toc}{section}{Books}
\printbibliography[heading=bibempty,type=book]




%----------------------------------------------------------------------------------------
%	INDEX
%----------------------------------------------------------------------------------------

\cleardoublepage % Make sure the index starts on an odd (right side) page
\phantomsection
\setlength{\columnsep}{0.75cm} % Space between the 2 columns of the index
\addcontentsline{toc}{chapter}{\textcolor{azure}{Index}} % Add an Index heading to the table of contents
\printindex % Output the index

%----------------------------------------------------------------------------------------
%	List of Listings
%----------------------------------------------------------------------------------------

\cleardoublepage % Make sure the index starts on an odd (right side) page
\phantomsection
\addcontentsline{toc}{chapter}{\textcolor{azure}{List of Listings}}
\lstlistoflistings

%----------------------------------------------------------------------------------------
%	List of Figures
%----------------------------------------------------------------------------------------

\cleardoublepage % Make sure the index starts on an odd (right side) page
\phantomsection
\addcontentsline{toc}{chapter}{\textcolor{azure}{List of Figures}}
\listoffigures

%----------------------------------------------------------------------------------------
%	List of Video lectures
%----------------------------------------------------------------------------------------

\chapter*{List of video lectures}
\addcontentsline{toc}{chapter}{\textcolor{azure}{Video lectures}}


\begin{itemize}
\item \href{https://www.youtube.com/watch?v=asGZTCR53KY&list=PL7vEgTL3FalY2eBxud1wsfz8OKvE9sd_z}{C++ Lecture 1 - The Standard Template Library}
\item \href{https://www.youtube.com/watch?v=iU3wsiJ5mts}{C++ Lecture 2 - Template Programming }
\item \href{https://www.youtube.com/watch?v=6PWUByLZO0g}{C++ Lecture 4 - Template Meta Programming}
\item \href{https://www.youtube.com/watch?v=Vck6kzWjY88}{The C++17 Parallel Algorithms Library and
Beyond}
\item \href{https://youtu.be/5vr7ItjyIH8}{Modern CUDA and C++}
\item \href{https://www.youtube.com/watch?v=Vck6kzWjY88}{The C++17 Parallel Algorithms Library and
Beyond}
\item \href{https://www.youtube.com/watch?v=js-e8xAMd1s}{The Asynchronous C++ Parallel Programming Model}
\end{itemize}



\end{document}
